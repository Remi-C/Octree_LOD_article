%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, Rémi Cura
%% 
%% This file is the Discussion of the result of the article
%% ---------------------------------------------------------------------


 \section{ Discussion }
	 \label{sec:discussion} 
	 
	 \subsection{Point cloud server}
	\label{par:pointcloudserver-limitation}
	We refer the reader to \cite{Cura2014} for an exhaustive analyse of the Point Cloud Server.
	Briefly, the PCS has demonstrated all the required capacitites to manage point clouds and scale well.
	To the best of our knowledge the fastest and easiest way to filter very big point cloud using complex spatial and temporal criteria, as well as natively integrate point cloud with other GIS data (raster, vector).
	The main limitation is based on the hypothesis than points can be regrouped into meaningful (regarding further point utilisation) patch. If this hypothesis is false, the PCS loose most of its interest.
	 
		 
	\subsection{Exploiting the order of points}
	From a practical point of view, implicitly storing the LOD using the point ordering
	seems to be estremely portable. Most softwares would not change the order of points.
	For those who might change the order of points, 
	it is still very easy to add the order as an attribute, thus making it fully portable.
	However, this approach has two limitations.
	The first limitation is that the order of point might already contains precious information. 
	For instance with a static Lidar device,
	the acquisition order allows to reconstruct neighbourhood information.
	The second limitation is that an LOD ordering might conflict with compression.
	Indeed ordering the points to form LOD will create a list of points were successive points are very different. Yet compressing works by exploiting similarities.
	A software like LasTool using delta compressing might suffer heavily from this. 
			 
	 \subsection{MidOc : an ordering for gradual geometrical approximation}
	 	We stress that the LOD are in fact almost continuous (as in the third illustrations of \ref{fig:banner_image}).
	 	\myimage{./illustrations/Objects/Objects_assembled.jpg}{All successive levels for common objects (car, window, 2 wheelers, light, tree, people, pole, ground), color is intensity for other points.}{fig:lod-common-objects} 
		 	
		MidOc is a way to order points based on their importance. In MidOc,
		the importance is defined geometrically.
		Yet specific applications may benefit from using other measure of importance, 
		possibly using other attributes than geometrical one,
		and possibly using more perceptual-oriented measures.
		
		MidOc relies on two hypothesis which might be false in some case. 
		Indeed, variation of density may be a wanted feature
		(e.g. stereovision, with more image on more important parts of the object being sense).
		Again the low geometrical noise hypothesis might be true for Lidar, but not for Stereo vision 
		or medical imaging point cloud. However in those case denoising methods may be applied before computing MidOc.

	\subsubsection{Applications}
		MidOc ordering might be of use in 3 types of applications. First it can be used for graphical LOD, as a service for point cloud visualisation.
		Second the ordering allows to correct density to be much more constant.
		Complex processing methods may benefits from an almost constant density, or for the absence of strong density variation.
		Third the ordering can be used for point cloud generalisation,
		as a service for processing methods that may only be able to deal with a fraction of the points. 
		
		The illustration \ref{fig:visual_LOD_left_right} gives visual example of LOD result and how it could be used to vary density depending on the distance to camera. 
		Figure \ref{fig:lod-common-objects} also gives visual examples for common objects of different dimensionality.
		It is visually clear that the rate of increase of points from LOD 0 to 4 for floor lamp (1D) window (2D) and tree (3D) is very different.
		Small details are also preserved like the poles or the antenna of the car. preserving those detail with random or distance based subsampling would be difficult.
		
		 
	\subsubsection{Implementation}
			%\subsubsection{Efficiency and performance}
		\label{subsubsec:bit_coordinates}
		Octree construction may be avoided by simply reading coordinates bitwise in a correctly centred/scaled point cloud.
		We centre a point cloud so that the lowest point of all dimension is $(0,0,0)$, and scale it so that the biggest dimension is in $[0,1[$.
		The point cloud is then quantized into $[0..2**L-1]$ for each coordinate.
		The coordinates are now integers, and for each point, reading its coordinates bitwise left to right gives the position of the point in the octree for level of the bit read.
		This means performing that this centring/scaling/quantization directly gives the octree. Moreover, further operations can be performed using bit arithmetic, which is extremely fast.
		\myimage{./illustrations/LOD/principle_of_binary_coordinate.png}{Principle of binary coordinates for a centered, scaled and quantized point cloud.}{fig:binary_coordinates_example}
		
		On this illustration the point $P$ has coordinates $(5,2)$ in a $[0,2^3-1]^2$ system. Reading the coordinates as binary gives $(b'101',b'010')$.
		Thus we now that on the first level of a quad tree, $P$ will be in the right (x=$b'1xx'$) bottom (y=$b'0yy'$) cell.
		For the next level, we divide the previous cell in 2, and read the next binary coordinate. $P$ will be in the left (x=$b'x0x'$) up (y=$b'y1y'$) cell. There is no computing required, only bit mask and data reading.
			
		Regarding implementation, the three we propose are much too slow, by an order of magnitude to be easily used in real situation. We stress however that the slowness comes from ineficient data manipulation, rather than from the complexity of the ordering. 
		It may also be possible to use the revert Hilbert ordering to directly compute MidOc.
		Furthermore, octree construction has been commonly done on GPU for more than a decade.
	 \subsubsection{Size versus LOD trade-off}
		 \label{point-cloud-server-troughput}
		 The table \ref{tab:lod-size-time} shows that using the level 3 may speed the transfer time by a 10 factor.
		 The point cloud server throughput is about 2-3 \mega $byte$ \per \second  (monoprocess),  sufficient for an internet troughput, but not fast enough for a LAN 10 \mega $byte$ \per \second.
		 This relatively slow troughput is due to current point cloud server limitation (cf \ref{par:pointcloudserver-limitation}).
	 \subsubsection{Large scale computing} 
		 The relatively slow computing (100 Millions points \per \second ) is a very strong limitation.
		 About $2/3$ of the computing time is spend on data transformation because algorithm works and output a list of points, which must then be desagregated/aggregated from/to a patch.
		 This could be avoided by a C implementation which can access raw patch, and would also be faster for ordering points.
		 
	 \subsubsection{LOD stream}
		 Streaming low level of detail patches greatly accelerate visualisation,
		 which is very useful when the full point cloud is not needed.
		 To further accelerate transmission, patch LOD can be determined according to the distance to camera. (See Figure \ref{fig:lod-dist-to-camera} for a naive visual explanation.)
		 \myimage{./illustrations/LOD/visual_result_distance_dependent.png}{Schematique example of LOD depending on distance to camera}{fig:lod-dist-to-camera}
		 \\
		 As seen before (See Section \ref{point-cloud-server-troughput}), the point cloud server is fast enough for an internet connection, but is currently slower than a file-based points streaming. Thus for the moment LOD stream is interesting only when bandwidth is limited.
	
	\subsection{Excessive Density detection and correction}
		\subsubsection{Fast detection}
		Density abnormality detection at the patch level offer the great advantage of avoiding to read points. This is the key to the speed of this method. We don't know any method that is as fast and simple.
		\\
		The limitations stems from the aggregated nature of patch. the number of points per patch doesn't give the density per point, but a quantized version of this per patch.
		So it is not possible to have a fine per point density.
		It also introduces the classical sampling limits : only density abnormalities spatially bigger than a patch  (at least $1 \meter$ wide for Paris) can be detected.
		
		\subsubsection{Simple correction}
		The correction of density peak we propose has the advantage of being instantaneous and not induce any data loss.
		It is also easy to use as safeguard for an application that may be sensible to density peak : the application simply defines the highest number of points \per \cubic \meter it can handle, and the Point cloud server will always output less than that.
		\\
		The most important limitation this method doesn't guarantee homogeneous density, only a maximum density.
		For instance if an application requires 1000 points \per \cubic \meter for ground patches, all the patches must have more than 1000 points, and patch must have been ordered with MidOc for level 0 to at least 5 ($4^5=1024$). 
		The homogeneous density may also be compromised when the patch are not only split spatially, but with other logics (in our case, points in patch can't be separated by more than 30 seconds, and all points in a patch must come from the same original acquisition file).
		
	\subsection{Crude dimensionality descriptor (MidOc by-product)}	  
			 
			 
		
			     
		
			 
	\subsection{Dimensionality descriptor and patch classification}
		 %\subsubsection{Ppl as a crude dimensionality descriptor 
		 %% @TODO
		 \todoall{parler des avantages/desavatnage comparé au descripteur classic, avec une petite illustration}
		 
		 
		 $ppl$ descriptor contains lots of information about the patch. This information is leveraged by the Random Forest method and permit a rough classification based on geometric differences.
		 As expected, $ppl$ descriptor are  not sufficient to correctly separate complex objects,
		 which is the main limitation for a classification application.
		 \\
		 Moreover, the descriptor can be used only if patches contain enough points,
		 which is not completely the case for distant tree patches for example.
	
	\subsection{Patch Classification}
		%\subsubsection{Introducing other features}
		The additional features are eextremely simple, and far from the one used in state of the art.
		Notably, we don't use any contextual feature.
		%\subsubsection{Classification Setting}
		We choose to classify directly in N classes, whereas due to the large unbalance indataset, cacade or 1 versus all approaches would be more adapted.
		
	\subsubsection{Analysing class hierarchy} 
		The figure \ref{fig:class-clustering-all-features} shows the limit of a classification without contextual information. For instance the class grid and buildings are similar because in Paris buildings balcony are typically made of grids.
		
		To better identify confusion between classes, we use a spectral layout on the affinity matrix.
		Graphing this matrix in 2D ammount to a problem of dimensionality reduction.
		It could use more advanced method than simply using the first two eigen vector,
		in perticular the two vector wouldn't need to be orthogonal (for instance, like in \cite{Hyvarinen2000})
				 
	\subsubsection{Classification results}
		  %\paragraph{Vosges data set}	
		  First the feature usage for vosges data set clearly shows that amongst all the simple descriptor, the $ppl$ descriptor is largely favored.
		  This may be explained by the fact that forest and bare land have very different dimensionality, which is conveyed by the $ppl$ descriptor.
		  
		  Second the patch classifier appears to have very good result to predict if a patch is forest or not. The precision is almost perfect for forest.
		  Because most of the errors are on border area, the recall for forest can also be easily artificially increased. The percent of points in patch that are in the patch class allow to compare result with a point based method. 
		  For instance the average precision per point for closed forest would be $0.99*0.883=0.874$ . We stress that this is averaged results, and better precision per point could be achieved because we may use random forest confidence to guess border area (with a separate learning for instance).
		  For comparison with point methods, the patch classifier predict with very good precision and support over 6 billions points in few seconds (few minutes for training). We don't know other method that have similar result while being as fast and natively 3D.
		   The Moor class can't be separated without more specialised descriptor, because Moor and no forest classes are geometrically very similar.
		  
		  The principal limitation is that for this kind of aerial Lidar data set the 2.5D approximation may be sufficient, which enables many raster based methods that may perform better or faster.
		  
		  %\paragraph{Paris data set}
		  The figure \ref{fig:result-paris} gives full results for paris data set, at various class hierarchy level.
		  Because the goal is filtering and not pure classification, we only comment the 7 classes result. The proposed methods appears very effective to find building, ground and trees.
		  Even taking into consideration the fact that patch may contains mixed classes (column mix.), the result are in the range of state of the art point classifier, while being extremely fast. 
		  This result are sufficient to increase recall or precision to 1 if necessary.
		  We stress that even results appearing less good (4+wheelers , 0.69 precision, 0.45 recall) are in fact sufficient to increase recall to 1 (by spatial dilatation of the result), which enables then to use more subtle methods on filtered patches.
		  
		  $ppl$ descriptor are less used than for the Vosges data set, but is still useful, particularly when there are few classes.
		  It is interesting to note that the mean intensity descriptor seems to be used to distinguish between objects, which makes it less useful in the 7 classes case.
		  The patch classifier for Paris data set is clearly limited to separate simple classes. In particular, the performances for objects are clearly lower than the state of the art. A dedicated approach should be used (cascaded or one versus all classifier). 
		  
		  %\paragraph{Timing consideration}
		  \todoall{comparer avec etat de l'art : article de Martin qu'il a envoyé et qui sont sur zotero}
			\todoall{penser à parler de \cite{Weinmann2015,Weinmann2015a,shapovalov2010} pour la comparaison de perf et de timings.}
			   
		 \subsubsection{Precision or Recall increase}
			 Because the propose methods are preprocess of filtering step, it can be advantageous to increase precision or recall.
			
			%\paragraph{Artificial increase of precision}
			\myimage{./illustrations/classif/result_paris/4_plus_wheelers_precision_vs_confidence.pdf}{Precision of 4+wheelers class is a roughly rising function of random forest confidence scores.}{fig:precision-vs-confidence} 
			In the figure \ref{fig:precision-increase} gives a visual example where increasing precision and reducing class heterogeneity is advantageous. This illustrates that having a $1$ precision or recall is not necessary the ultimate goal.
			In this case it would be much easier to perform line detection starting from red patches rather than blue patches.
			\\
			The limitation of artificial precision increase is 
			that it is only possible when precision is roughly a rising function of random forest confidence, as seen on the illustration \ref{fig:precision-vs-confidence}.
			For this class, by accepting only prediction of random forest that have a confidence over $0.3$ the precision goes from $0.68$ to $0.86$, at the cost of ignoring half the predictions for this class.
			This method necessitates that the precision is roughly a rising function of the confidence, as for the 4+wheeler class for instance (See Figure \ref{fig:precision-vs-confidence}).   
			This strategy is not possible for incoherent class, like unclassified class.
 
			%\paragraph{Filtering: artificial increase of recall}
			The method we present for artificial recall increase is only possible if at least one patch of each object is retrieved, and objects are spatially dense.
			This is because a spatial dilatation operation is used.
			This is the case for "4+wheelers" objects in the paris data set for instance.
			The whole method is possible because spatial dilatation is very fast in point cloud server (because of index).
			Moreover, because the global goal is to find all patches of a class while leaving out some patches,
			it would be senseless to dilate with a very big distance.
			In such case recall would be $1$, but all patches would be in the result, thus there would be no filtering, and no speeding.
			\\
			The limitation is that this recall increase method is more like a deformation of already correct results 
			rather than a magical method that will work with all classes.
				