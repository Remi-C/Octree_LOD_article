%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, RÃ©mi Cura
%% 
%% This file is the Discussion of the result of the article
%% ---------------------------------------------------------------------


 \section{ Discussion }
	 \label{sec:discussion} 
	 
	 \subsection{Point cloud server}
	\label{par:pointcloudserver-limitation}
	We refer the reader to \cite{Cura2014} for an exhaustive analyse of the Point Cloud Server.
	Briefly, the PCS has demonstrated all the required capacitites to manage point clouds and scale well.
	To the best of our knowledge the fastest and easiest way to filter very big point cloud using complex spatial and temporal criteria, as well as natively integrate point cloud with other GIS data (raster, vector).
	The main limitation is based on the hypothesis than points can be regrouped into meaningful (regarding further point utilisation) patch. If this hypothesis is false, the PCS loose most of its interest.
	 
		 
	\subsection{Exploiting the order of points}
	From a practical point of view, implicitly storing the LOD using the point ordering
	seems to be estremely portable. Most softwares would not change the order of points.
	For those who might change the order of points, 
	it is still very easy to add the order as an attribute, thus making it fully portable.
	However, this approach has two limitations.
	The first limitation is that the order of point might already contains precious information. 
	For instance with a static Lidar device,
	the acquisition order allows to reconstruct neighbourhood information.
	The second limitation is that an LOD ordering might conflict with compression.
	Indeed ordering the points to form LOD will create a list of points were successive points are very different. Yet compressing works by exploiting similarities.
	A software like LasTool using delta compressing might suffer heavily from this. 
			 
	 \subsection{MidOc : an ordering for gradual geometrical approximation}
	 	We stress that the LOD are in fact almost continuous (as in the third illustrations of \ref{fig:banner_image}).
	 	\myimage{./illustrations/Objects/Objects_assembled.jpg}{All successive levels for common objects (car, window, 2 wheelers, light, tree, people, pole, ground), color is intensity for other points.}{fig:lod-common-objects} 
		 	
		MidOc is a way to order points based on their importance. In MidOc,
		the importance is defined geometrically.
		Yet specific applications may benefit from using other measure of importance, 
		possibly using other attributes than geometrical one,
		and possibly using more perceptual-oriented measures.
		
		MidOc relies on two hypothesis which might be false in some case. 
		Indeed, variation of density may be a wanted feature
		(e.g. stereovision, with more image on more important parts of the object being sense).
		Again the low geometrical noise hypothesis might be true for Lidar, but not for Stereo vision 
		or medical imaging point cloud. However in those case denoising methods may be applied before computing MidOc.

	\subsubsection{Applications}
		MidOc ordering might be of use in 3 types of applications. First it can be used for graphical LOD, as a service for point cloud visualisation.
		Second the ordering allows to correct density to be much more constant.
		Complex processing methods may benefits from an almost constant density, or for the absence of strong density variation.
		Third the ordering can be used for point cloud generalisation,
		as a service for processing methods that may only be able to deal with a fraction of the points. 
		
		The illustration \ref{fig:visual_LOD_left_right} gives visual example of LOD result and how it could be used to vary density depending on the distance to camera. 
		Figure \ref{fig:lod-common-objects} also gives visual examples for common objects of different dimensionality.
		It is visually clear that the rate of increase of points from LOD 0 to 4 for floor lamp (1D) window (2D) and tree (3D) is very different.
		Small details are also preserved like the poles or the antenna of the car. preserving those detail with random or distance based subsampling would be difficult.
		
		 
	\subsubsection{Implementation}
			%\subsubsection{Efficiency and performance}
		\label{subsubsec:bit_coordinates}
		Octree construction may be avoided by simply reading coordinates bitwise in a correctly centred/scaled point cloud.
		We centre a point cloud so that the lowest point of all dimension is $(0,0,0)$, and scale it so that the biggest dimension is in $[0,1[$.
		The point cloud is then quantized into $[0..2**L-1]$ for each coordinate.
		The coordinates are now integers, and for each point, reading its coordinates bitwise left to right gives the position of the point in the octree for level of the bit read.
		This means performing that this centring/scaling/quantization directly gives the octree. Moreover, further operations can be performed using bit arithmetic, which is extremely fast.
		\myimage{./illustrations/LOD/principle_of_binary_coordinate.png}{Principle of binary coordinates for a centered, scaled and quantized point cloud.}{fig:binary_coordinates_example}
		
		On this illustration the point $P$ has coordinates $(5,2)$ in a $[0,2^3-1]^2$ system. Reading the coordinates as binary gives $(b'101',b'010')$.
		Thus we now that on the first level of a quad tree, $P$ will be in the right (x=$b'1xx'$) bottom (y=$b'0yy'$) cell.
		For the next level, we divide the previous cell in 2, and read the next binary coordinate. $P$ will be in the left (x=$b'x0x'$) up (y=$b'y1y'$) cell. There is no computing required, only bit mask and data reading.
			
		Regarding implementation, the three we propose are much too slow, by an order of magnitude to be easily used in real situation. We stress however that the slowness comes from ineficient data manipulation, rather than from the complexity of the ordering. 
		It may also be possible to use the revert Hilbert ordering to directly compute MidOc.
		Furthermore, octree construction has been commonly done on GPU for more than a decade.
	 \subsubsection{Size versus LOD trade-off}
		 \label{point-cloud-server-troughput}
		 The table \ref{tab:lod-size-time} shows that using the level 3 may speed the transfer time by a 10 factor.
		 The point cloud server throughput is about 2-3 \mega $byte$ \per \second  (monoprocess),  sufficient for an internet troughput, but not fast enough for a LAN 10 \mega $byte$ \per \second.
		 This relatively slow troughput is due to current point cloud server limitation (cf \ref{par:pointcloudserver-limitation}).
	 \subsubsection{Large scale computing} 
		 The relatively slow computing (100 Millions points \per \second ) is a very strong limitation.
		 About $2/3$ of the computing time is spend on data transformation because algorithm works and output a list of points, which must then be desagregated/aggregated from/to a patch.
		 This could be avoided by a C implementation which can access raw patch, and would also be faster for ordering points.
		 
	 \subsubsection{LOD stream}
		 Streaming low level of detail patches greatly accelerate visualisation,
		 which is very useful when the full point cloud is not needed.
		 To further accelerate transmission, patch LOD can be determined according to the distance to camera. (See Figure \ref{fig:lod-dist-to-camera} for a naive visual explanation.)
		 \myimage{./illustrations/LOD/visual_result_distance_dependent.png}{Schematique example of LOD depending on distance to camera}{fig:lod-dist-to-camera}
		 \\
		 As seen before (See Section \ref{point-cloud-server-troughput}), the point cloud server is fast enough for an internet connection, but is currently slower than a file-based points streaming. Thus for the moment LOD stream is interesting only when bandwidth is limited.
	
	\subsection{Excessive Density detection and correction}
		\subsubsection{Fast detection}
		Density abnormality detection at the patch level offer the great advantage of avoiding to read points. This is the key to the speed of this method. We don't know any method that is as fast and simple.
		\\
		The limitations stems from the aggregated nature of patch. the number of points per patch doesn't give the density per point, but a quantized version of this per patch.
		So it is not possible to have a fine per point density.
		It also introduces the classical sampling limits : only density abnormalities spatially bigger than a patch  (at least $1 \meter$ wide for Paris) can be detected.
		
		\subsubsection{Simple correction}
		The correction of density peak we propose has the advantage of being instantaneous and not induce any data loss.
		It is also easy to use as safeguard for an application that may be sensible to density peak : the application simply defines the highest number of points \per \cubic \meter it can handle, and the Point cloud server will always output less than that.
		\\
		The most important limitation this method doesn't guarantee homogeneous density, only a maximum density.
		For instance if an application requires 1000 points \per \cubic \meter for ground patches, all the patches must have more than 1000 points, and patch must have been ordered with MidOc for level 0 to at least 5 ($4^5=1024$). 
		The homogeneous density may also be compromised when the patch are not only split spatially, but with other logics (in our case, points in patch can't be separated by more than 30 seconds, and all points in a patch must come from the same original acquisition file).
		
	\subsection{Crude dimensionality descriptor (MidOc by-product)}	 
		\myimage{"./illustrations/comparing_dim_desc/hist_comparison_for_tree"}{Histogram of $Dim_{LOD}$ and $Dim_{cov}$ for patch in trees.}{fig:hist_comparison_for_tree} 
		
		Tree patches are challenging for both dimensionality descriptor.
		There possible dimension changes a lot (See Fig. \ref{fig:hist_comparison_for_tree}), although $Dim_{LOD}$ is more concentrated.
		Yet, $ppl$ is extremely useful to classify trees.
		Indeed, $ppl$ contains the dimensionality at various scale, and potentially the variation of it, which is quite specific for trees (See Fig. \ref{fig:analysing_tree}).
		  
		\myimage{"./illustrations/comparing_dim_desc/analysing_tree/analysing_tree"}{Evolution of tree patch octree cells occupancy, illustrating the various dimensions of trees depending on the scale.}{fig:analysing_tree} 
			
		We stress that a true octree cell occupancy (i.e. without picking points as in the $ppl$) can be obtained without computing the octree, simply by using the binary coordinates (See \ref{fig:binary_coordinates_example}).
		
		Overall, $ppl$ offers a good alternative to the classical dimensionality descriptor ($Dim_{cov}$), being more robust and multiscale. 
		However the $ppl$ also has limitations. 
		First the quality of the dimensionality description may be affected by a low number of points in the patch. 
		Second in some case it is hard to reduce it to a meaningful $Dim_{LOD}]$.
		Last because of assumption on density, it is sensible to geometric noise.
			  
	\subsection{Patch Classification}
		$ppl$ descriptor contains lots of information about the patch. This information is leveraged by the Random Forest method and permit a rough classification based on geometric differences.
		As expected, $ppl$ descriptor are  not sufficient to correctly separate complex objects,
		which is the main limitation for a classification application. 
		
		%\subsubsection{Introducing other features}
		The additional features are extremely simple, and far from the one used in state of the art.
		Notably, we don't use any contextual feature.
		%\subsubsection{Classification Setting}
		We choose to classify directly in N classes, whereas due to the large unbalance indataset, cacade or 1 versus all approaches would be more adapted.
		
	\subsubsection{Analysing class hierarchy} 
		The figure \ref{fig:class-clustering-all-features} shows the limit of a classification without contextual information. For instance the class grid and buildings are similar because in Paris buildings balcony are typically made of grids.
		
		To better identify confusion between classes, we use a spectral layout on the affinity matrix.
		Graphing this matrix in 2D ammount to a problem of dimensionality reduction.
		It could use more advanced method than simply using the first two eigen vector,
		in perticular the two vector wouldn't need to be orthogonal (for instance, like in \cite{Hyvarinen2000})
				 
	\subsubsection{Classification results}
		  %\paragraph{Vosges data set}	
		  First the feature usage for vosges data set clearly shows that amongst all the simple descriptor, the $ppl$ descriptor is largely favored.
		  This may be explained by the fact that forest and bare land have very different dimensionality, which is conveyed by the $ppl$ descriptor.
		  
		  Second the patch classifier appears to have very good result to predict if a patch is forest or not. The precision is almost perfect for forest.
		  Because most of the errors are on border area, the recall for forest can also be easily artificially increased. The percent of points in patch that are in the patch class allow to compare result with a point based method. 
		  For instance the average precision per point for closed forest would be $0.99*0.883=0.874$ . We stress that this is averaged results, and better precision per point could be achieved because we may use random forest confidence to guess border area (with a separate learning for instance).
		  For comparison with point methods, the patch classifier predict with very good precision and support over 6 billions points in few seconds (few minutes for training). We don't know other method that have similar result while being as fast and natively 3D.
		   The Moor class can't be separated without more specialised descriptor, because Moor and no forest classes are geometrically very similar.
		  
		  The principal limitation is that for this kind of aerial Lidar data set the 2.5D approximation may be sufficient, which enables many raster based methods that may perform better or faster.
		  
		  %\paragraph{Paris data set}
		  The figure \ref{fig:result-paris} gives full results for paris data set, at various class hierarchy level.
		  Because the goal is filtering and not pure classification, we only comment the 7 classes result. The proposed methods appears very effective to find building, ground and trees.
		  Even taking into consideration the fact that patch may contains mixed classes (column mix.), the result are in the range of state of the art point classifier, while being extremely fast. 
		  This result are sufficient to increase recall or precision to 1 if necessary.
		  We stress that even results appearing less good (4+wheelers , 0.69 precision, 0.45 recall) are in fact sufficient to increase recall to 1 (by spatial dilatation of the result), which enables then to use more subtle methods on filtered patches.
		  
		  $ppl$ descriptor are less used than for the Vosges data set, but is still useful, particularly when there are few classes.
		  It is interesting to note that the mean intensity descriptor seems to be used to distinguish between objects, which makes it less useful in the 7 classes case.
		  The patch classifier for Paris data set is clearly limited to separate simple classes. In particular, the performances for objects are clearly lower than the state of the art. A dedicated approach should be used (cascaded or one versus all classifier). 
	  
	  
	  \subsubsection{Estimating the speed and performance of patch based classification compared to point based classification}
		  %\paragraph{Timing consideration}
		  The Point Cloud Server is designed to work on patches, which in turns enable massive scaling.
		  
		  %\paragraph{speed}
		  Timing a server is difficult because of different layer of caches, and background workers. Therefore, timing should be considered as order of magnitude.
		  
		  For Paris data set,extracting extra classification features requires $\sim \frac{400 \second},{n_{workers}}$( 1 to 8 workers), learning $\sim 210 \second$,
		  and classification $\sim$ few \second.
		  We refer to \cite{Weinmann2015}(Table 5) for point classification timing on the same dataset (4.28\hour, 2\second,90\second ) (please note that the training set is much reduced by data balancing, whereas we keep the full training set and use weight for balancing).
		  As expected the speed gain is high for complex feature computing (not required) and point classification (done on patch and not points in our case).
		  
		  For Vosges data set, features are extracted at $1 \mega pts \per \second \per worker$, learning $\sim few \minute$, classification $\sim 10 \second$.
		  The Vosges data set has not been used in other articles, therefore we propose to compare the timings to \cite{shapovalov2010} (Table 3). Keeping only feature computation and random forest training (again on a reduced data set), they process 2 \mega points in 2 \minute, whereas our method process the equivalent of 5.5 B points in few minutes.
		  
		  
		  Learning and classification are monothreaded (3 folds validation), although the latter is easy to parallelise.
		  Overall, the proposed method is one to three orders of magnitude faster.
		  %\paragraph{performance}
		  
		  For Paris data set (Fig. \ref{fig:result-paris}), we compare to \cite{Weinmann2015}(Table 5). As expected there results are better, particularly in terms of precision (except for the class of vegetation). This trend is aggravated by taking into account the "mix." factor.
		  Indeed we perform patch classification, and patch may not pertain to only one class, which is measured by the mix factor (amount of points in the main class divided by the total number of point).
		  However, including the mix factor the results are still within the 85 to 90 \% precision for the main classes (ground, building, natural).
		  
		  For Vosges data set (Fig \ref{fig:result-vosges}), we refer to \cite{shapovalov2010} (Table 2). There random forest classifier get trees with 93\% precision and  89\% recall.
		  Including the mix factor we get trees with a precision of 87\% and 80\% recall.
		  As a comparison to image based classification, an informal experiment of classification with satellite image reaches between 85 \% and 93 \% of precision for forest class depending on the pixel size (between 5 and 0.5 \metre).
		  
		  Overall, the proposed method get reasonably good results compared to more sophisticated methods,
		  while being much faster.
		  It so makes a good candidate as a preprocessing filtering step.
		  
	 \subsubsection{Precision or Recall increase}
	 Because the propose methods are preprocess of filtering step, it can be advantageous to increase precision or recall.
		
		%\paragraph{Artificial increase of precision}
		\myimage{./illustrations/classif/result_paris/4_plus_wheelers_precision_vs_confidence.pdf}{Precision of 4+wheelers class is a roughly rising function of random forest confidence scores.}{fig:precision-vs-confidence} 
		In the figure \ref{fig:precision-increase} gives a visual example where increasing precision and reducing class heterogeneity is advantageous. This illustrates that having a $1$ precision or recall is not necessary the ultimate goal.
		In this case it would be much easier to perform line detection starting from red patches rather than blue patches.
		\\
		The limitation of artificial precision increase is 
		that it is only possible when precision is roughly a rising function of random forest confidence, as seen on the illustration \ref{fig:precision-vs-confidence}.
		For this class, by accepting only prediction of random forest that have a confidence over $0.3$ the precision goes from $0.68$ to $0.86$, at the cost of ignoring half the predictions for this class.
		This method necessitates that the precision is roughly a rising function of the confidence, as for the 4+wheeler class for instance (See Figure \ref{fig:precision-vs-confidence}).   
		This strategy is not possible for incoherent class, like unclassified class.

		%\paragraph{Filtering: artificial increase of recall}
		The method we present for artificial recall increase is only possible if at least one patch of each object is retrieved, and objects are spatially dense.
		This is because a spatial dilatation operation is used.
		This is the case for "4+wheelers" objects in the paris data set for instance.
		The whole method is possible because spatial dilatation is very fast in point cloud server (because of index).
		Moreover, because the global goal is to find all patches of a class while leaving out some patches,
		it would be senseless to dilate with a very big distance.
		In such case recall would be $1$, but all patches would be in the result, thus there would be no filtering, and no speeding.
		\\
		The limitation is that this recall increase method is more like a deformation of already correct results 
		rather than a magical method that will work with all classes.
				