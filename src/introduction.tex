%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, RÃ©mi Cura
%% 
%% This file contains the introduction of article
%% ---------------------------------------------------------------------


\section{Introduction}

\begin{figure*}[t!]
	\begin{center}
		\fbox{\includegraphics[width=\textwidth,keepaspectratio ]{./illustrations/chap2/lod_banner/banner_for_paper}}
		\caption{Graphical Abstract : a Lidar point cloud (1), is split it into patches (2) 
		and stored in the PCS (\cite{Cura2015}), patches are re-ordered to obtain free LOD 
		(3 : a gradient of LOD here).
		Lastly the ordering is used as a feature for learning and efficient filtering (4) } 
		\label{lod.fig:banner_image}
	\end{center}
\end{figure*} 

\subsection{Problem}  
	Point cloud data is becoming more and more common. Following the same trend, the acquisition frequency and precision of the Lidar device are also increasing.
	Thus point cloud processing is entering in the Big Data realm.
	Point cloud data usage is also spreading and going out of their traditional user communities. 
	Lidar are now commonly used by non-specialized users. 
	The now common datasets in the multi billion point range can not fit in computer memory. 
	Furthermore, having the complete and fully detailed point cloud is impracticable, unnecessary, or even damageable for most applications.
	
	The ability to reduce the number of points is then vital for practical point cloud management and usage.
 
	\myimage{"./illustrations/chap2/two_reduction_strategy/two_reduction_strategy"}{Two strategies to limit the amount of points to work on.}{lod.fig:two_reduction} 
	
	There are basically two levers to reduce the amount of data considered (See Figure \ref{lod.fig:two_reduction}). The first would be \textbf{filtering} data based on characteristics (position, time, semantic, etc.), thus keeping the original data, but only a portion of it.
	The second lever would be to \textbf{generalise} the data, that is replace many points with few objects that represent them well. Those objects can be more abstract (geometric primitives like plane for example), or of the same type, i.e. simply some well chosen points (subset).
	
	For instance, to visualize the parts of a massive point cloud, we might fetch only the points that are visible (filtering),
	and not display the totality of selected points, but only points better representing the scene (generalisation).
	
	The Point Cloud Server (PCS) we propose un \cite{cura2015} is compatible with filtering and generalization. It extensively covers the filtering part, with many possibilities (spatial, semantic, attributes, using vector and raster data, using metadata). The PCS also use geenralization of points in the form of more abstract types (bounding box, planes, stats, etc.).
	In this article, we explore how to generalise groups of points by choosing a representative subset of points.
	
	This kind of problem is common in Geographical Information System (GIS): how to generalize information contained in huge datasets, that is reduce the details of data while retaining its principal organisation.
	
	This problem is also much broader, very commons across research field, and could be seen as compression, clustering, dimensionality reduction, or Level Of Detail (LOD) if the goal is visualisation.
	
	In this article, we consider reducing the amount of points in several levels while preserving the geometric characteristics of the underlying sensed object, in an efficient manner, and while being robust to the way sensing was done. The goal is not only visualization of large point cloud, but mainly processing of large point clouds.
	
	Robustness to variation of density is necessary because the sensing may be structured for the sensing device (for instance a Lidar may sense point using a constant angle), but not necessary for the sensed object (see figure \ref{lod.fig:irregular_sampling})
	\myimage{"./illustrations/chap2/problem_in_sampling/regular_vs_irregular_sampling"}{Regular sensing does not imply regular sampling.}{lod.fig:irregular_sampling}.
	

\subsection{Related Work} 

	Sophisticated methods have been proposed to generalise points 2D points for cartographic applications. \cite{Sester2001} uses Self Organizing Maps, \cite{Schwartges2013} uses Active Range and (mixed) integer
	linear programming.
	In both case, the goal is a very specific type of visualisation (cartography), and it obviously relies on having a 2D plan on which perform the methods.
	Applying such methods to point clouds would thus require to have access to surfaces of sensed objects.
	
	Yet, getting this surface (reconstruction) is a very hard challenge, sometime not even possible, and thus we can not consider to have access to it.
	
	Because the goal si to produce hierarchical levels of points, it seems natural to use a hierarchical structure to compute those levels.
	\cite{Rusinkiewicz2000} use a Bounding Sphere Hierarchy for a visualisation application.
	On the other hand, Octree (\cite{Meagher1982}) have become the de-facto choice.
	Indeed, they can be build efficiently ordering points by Morton order (\cite{Feng2014}),
	which is similar in principle to GeoHash (\cite{Sabo2014}).
	Octree can even be created out of memory for extremely large point clouds (\cite{Baert2014}). 
	
	Moreover, their regularity allows efficient representation and compression (\cite{Schnabel2006,Huang2006}), as well as fast geospatial access to data (\cite{Elseberg2013}).
	Octree are also natural condidates to nesting (i.e. create a hierarchy of octrees with various resolution and occupancy, as in \cite{Hornung2013}). 
	
	
	\cite{Bereuter2015} recently gave an overview of how quad tree can be used for point generalisation.
	The steps are first to compute a tree for the point cloud.
	Then, the point generalisation at a given level is obtained for each cell of the same tree leve, by having one point represent all the points of the cell.
	
	There are two methods to choose a point representing the others. The first one is to select on points among all ('select').
	The second method is to create a new point that will represent well the others ('aggregate'). 
	Both these methods can use geometry as well as other attributes.
	
	In theory, choosing an optimal point would also depend on application!
	For instance lets consider a point cloud containing a classification, and suppose the application is to identify the presence of a very rarely present class C.
	In this case a purely geometrical LOD would probably hide C until the very detailed levels, as such the optimal choosing strategy would need to depend on this classification attributes.
	
	However,a LOD method has to be agnostic regarding point clouds,
	and point clouds may have many attributes of various type and meaning, as long as many applications.
	Therefore, most methods use only the minimal common factor of possible attributes, that is spatial coordinates. 
	For visualisation applications, aggregating points seems to be the most popular choice \cite{Schutz2015,Hornung2013,Elseberg2013}. with aggregating functions like centroids of the points or centroid of the cell.
	
	All of this methods also use an aggregative function (barycentre of the points, centroid of the cell) to represent the points of a cell.
	Using the barycentre seems intuitive, as it is also the point that minimize the squared distance to other points in the cell, and thus a kind of geometric error.
	
	However, using the 'aggregate' rather than 'select' strategy necessary introduces aggregating errors
	 (as opposed to potential aliasing error), and is less agnostic.
	This might not be a problem for visualization application.
	Yet our goal is to provide LOD for other processing methods, which might be influenced by aggregating errors.
	Furthermore, the barycentre is very sensible to density variations.
	
	Therefore, we propose to use a 'select' strategy. The point to be selected is the closest to the centroid of the octree cell.
	In most case, the point cloud density is sufficient, and this strategy gives a nearly regularly sampled point cloud, which might be a statistical advantage for processing methods (forming a Latin Hypercube, see \cite{McKay1979}).
	
	

\subsection{Contribution}
	This paper re-uses and combines existing and well established methods with a focus on simplicity and efficiency. As such, all the methods are tested on billions scale point cloud, and are Open Source for sake of reproducibility test and improvements.
	We propose a simple method that enables portable, computation-free, geometrical Level Of Detail.
	Our first contribution is to propose to store the LOD information directly into the ordering of points rather than externally, avoiding any data duplication.
	Thus, we don't duplicate information, and the more we read points, the more precise of an approximation of the point cloud we get. If we read all the points, we have the original point cloud.
	
	The second contribution is a simple way to order points in order to have an increasingly better geometric approximation of the point cloud when following this order.
	
	The third contribution is to show that this ordering embed information about the dimensionality of the sensed object,
	to the point of being a simple and free dimensionality descriptor.
	We demonstrate the interest of this descriptor by performing a Random Forest classification that can then be used for very fast pre-filtering of points, and other applications.
		
	
\subsection{Plan of the article}
	The rest of this article is organized as follows:
	in the next section \ref{lod.sec:method} we present the methods.  
	In the result section \ref{lod.sec:result} we give the results.
	We discuss it and the possible limitations in section \ref{lod.sec:discussion}.
	
	Following the IMRAD format~\citep{Wu2011}, the remainder of this article is divided into three sections.
	Section~\ref{lod.sec:method} presents the LOD solution, how it produces a dimensionality descriptor, and how this can leveraged for classification.  
	Section~\ref{lod.sec:result} reports on the experiments validating the methods.
	Finally, the details, the limitations, and potential applications are discussed in Section~\ref{lod.sec:discussion}.
	
