%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, Rémi Cura
%% 
%% This file contains the introduction of article
%% ---------------------------------------------------------------------


\todoall{utiliser point-set à la place de point cloud}

\section{Introduction}

\begin{figure*}[t!]
	\begin{center}
		\fbox{\includegraphics[width=\textwidth,keepaspectratio ]{./illustrations/chap2/lod_banner/banner_for_paper}}
		\caption{Graphical Abstract : a Lidar point cloud (1), is split it into patches (2) 
		and stored in the PCS (\cite{Cura2015}), patches are re-ordered to obtain free LOD 
		(3) (a gradient of LOD here), lastly the ordering by-product is a multiscale dimensionality descriptor used as a feature for learning and efficient filtering (4).} 
		\label{lod.fig:banner_image}
	\end{center}
\end{figure*} 

\subsection{Problem}   
	Democratisation of sensing device have resulted into an expansion of acquired point clouds.
	In the same time, acquisition frequency and precision of the Lidar device are also increasing,
	resulting in an explosion of number of points.
	
	
	Datasets are now commonly in the multi billion point range, leading to practical issue to store and use them.
	Moreover, point cloud data usage is more common and no more limited to a specialized community. 
	Non specialised users require easy access to data.
	By necessitating easy access and storage and processing for a large amount of data, point clouds are entering the Big Data realm.
	
	Yet all those data are not always needed; having the complete and fully detailed point cloud is impracticable, unnecessary, or even damageable for most applications.
	Therefore, the ability to reduce the number of points is a key point for practical point cloud management and usages.
	
	The number of points must not only be reduced, but often the density corrected.
	Indeed, point clouds from Lidar do not have a constant density.
	The sensing may be structured for the sensing device (for instance a Lidar may sense point using a constant angle), but not necessary for the sensed object (see Fig. \ref{lod.fig:irregular_sampling}). Furthermore,fusing multiple point clouds also produce non regular density.
	\myimageHL{"./illustrations/chap2/problem_in_sampling/regular_vs_irregular_sampling"}{Regular sensing does not imply regular sampling.}{lod.fig:irregular_sampling}.
	
	There are basically two approaches to reduce the amount of data considered (See Figure \ref{lod.fig:two_reduction}).
	The first is to use a \textbf{filtering} strategy based on data characteristics (position, time, semantic, etc.) which keeps only a portion the original data.
	The second is a \textbf{generalisation} strategy, where we replace many points with fewer objects that represent appropriately those points. 
	For instance, in order to visualize massive point cloud, it's important to fetch only the appropriate points by selecting the ones which are visible (filtering) and which are the most representative of the scene (generalisation) at the same time.
	 
	Many methods perform filtering, usually by using simple spatial criteria (for instance, points in polygon).
	Generalisation is also popular in its most basic form (generalise points by points).
	\cite{Cura2015} covers extensively filtering with many possibilities (spatial, semantic, attributes, using vector and raster data, using metadata), and also proposes generalisation.
	Nevertheless it uses a generalisation approach only based on more abstract types (bounding box, planes, stats, etc.), which limits its use to methods that are adapted to those types.
	It does not generalise points by points.
	 
	In this work we propose to extend the PCS to explore the generalisation of groups of points by choosing a representative subset of points (See Fig. \ref{lod.fig:two_reduction}).
	 	
	 
	We propose to use Level Of Details that reduce successively the number of points while preserving the geometric characteristics of the underlying sensed object.
	Our method is designed to be efficient, robust to point density variation and can be used for many large point clouds processing, including visualisation.
	
	\myimageHL{"./illustrations/chap2/two_reduction_strategy/two_reduction_strategy"}{Two strategies to limit the amount of points to work on.}{lod.fig:two_reduction}{0.5}
	
	
\subsection{Related Work} 

	Finding a subset of point that represents well all the points is a common problem.
	It has been extensively studied in Geographical Information System (GIS)and other research field.
	It could be seen as compression, clustering, dimensionality reduction, or Level Of Detail (LOD) for visualisation.
	

	Sophisticated methods have been proposed to generalise 2D points for cartographic applications (\cite{Sester2001}, \cite{Schwartges2013}).
	Yet those methods are limited to 2D points, and could not be easily modified to work in 3D.
	Indeeed, those methods are cartographic by nature, which means that they rely on having all the points on a simple surface : the 3D plan formed by the map.
	Applying directly such methods to point clouds would thus require to have access to surfaces of sensed objects.
	Yet, getting this surface (reconstruction) is a very hard challenge, sometime without solution, and thus we can not rely on it.
	For those limitation and large computing cost, those advanced methods can not be used for large 3D point clouds. 
	
	
	Other much simpler methods have been designed to work on 3D points. 
	Because the goal is to produce hierarchical levels of points, it seems natural to use a hierarchical structure to compute those levels.
	The main idea is to build a hierarchy of volumes, then each level of the hierarchy corresponds to a LOD. For each volume, a point is created/chosen to generalise all the points within the volume.
	\cite{Rusinkiewicz2000} use a Bounding Sphere Hierarchy for a visualisation application.
	Yet spheres are not well adapted to represent planes, which form a large part of man-made objects and structures.
	On the other hand, Octree (\cite{Meagher1982}) have become the de-facto choice.
	It seems that the most popular use of Octree is as spatial acceleration structure (spatial index).
	Octree have several advantages.
	The first is that their basic nature is closely related to Morton (or GeoHash) order,
	making them efficient to build (\cite{Sabo2014}, \cite{Feng2014}).
	They can also be created out of memory for extremely large point clouds (\cite{Baert2014}). 
	Moreover, their regularity allows efficient representation and compression (\cite{Schnabel2006,Huang2006}), as well as fast geospatial access to data (\cite{Elseberg2013}).

	Octree are also natural condidates to nesting (i.e. create a hierarchy of octrees with various resolution and occupancy, as in \cite{Hornung2013}).
   	Octree construction into file system hierarchy approach is still popular today (\cite{OscarMartinez-Rubi2015}), with point cloud in the 600 Billions points range.
   	It has also been adapted to distributed file system (cloud-computing)  \footnote{\url{https://github.com/connormanning/entwine}}, with processing of 100 Billions points at 2 Billions pts \per hour using a 32 cores 64 GB computer.
	
	
	However, the method using Octrees present several disadvantages. 
	Each method uses a custom octree format that is most often stored in an external file.
	This raises problems of concurrency and portability. 

	There a several ways to use an Octree to generalise points.
	We could not find a study of those ways for 3D points. 
	However, \cite{Bereuter2015} recently gave an overview of how quad tree can be used for point generalisation.
	Quad trees are 2D Octrees, yet \cite{Bereuter2015} analyse can be directly translated in 3D. 
 
	The steps are first to compute a tree for the point cloud.
	Then, the point generalisation at a given level is obtained for each cell of the same tree level, by having one point represent all the points of this cell.
	
	There are two methods to choose a point representing the others. The first one is to select on points among all ('select').
	The second method is to create a new point that will represent well the others ('aggregate'). 
	Both these methods can use geometry of points, but also other attributes.
	
	In theory, choosing an optimal point would also depend on application.
	For instance lets consider a point cloud containing a classification, and suppose the application is to visually identify the presence of a very rarely present class C.
	In this case a purely geometrical LOD would probably hide C until the very detailed levels. On the opposite, prefering a point classified in C whenever possible would be optimal for this application.
	
	However, a LOD method has to be agnostic regarding point clouds,
	and point clouds may have many attributes of various type and meaning, as long as many applications.
	Therefore, most methods use only the minimal common factor of possible attributes, that is spatial coordinates. 
	For visualisation applications, aggregating points seems to be the most popular choice \cite{Schutz2015,Hornung2013,Elseberg2013}. with aggregating functions like centroids of the points or centroid of the cell.
	
	All of this methods also use an aggregative function (barycentre of the points, centroid of the cell) to represent the points of a cell.
	Using the barycentre seems intuitive, as it is also the point that minimize the squared distance to other points in the cell, and thus a measure of geometric error.
	
	However, using the 'aggregate' rather than 'select' strategy necessary introduces aggregating errors
	 (as opposed to potential aliasing error), and is less agnostic.
	Indeed, aggregating means fabricating new points, and also necessitate a way to aggregate for each attributes, which might be complex (for instance semantic aggregating; a point of trash can and a point of bollard could be aggregated into a point of street furniture).
	This might not be a problem for visualization application.
	Yet our goal is to provide LOD for other processing methods, which might be influenced by aggregating errors.
	Furthermore, the barycentre is very sensible to density variations.
	
	Therefore, we prefer to use a 'select' strategy. The point to be selected is the closest to the centroid of the octree cell.
	If the point cloud density is sufficient this strategy produces a nearly regularly sampled point cloud, which might be a statistical advantage for processing methods. 
	To establish a parallel with statistics, picking one point per cell is akin to a Latin Hypercube (see \cite{McKay1979}).
	Avoiding the averaging strategy might also increase the quantity of information than can be retrieved (similar to compressed sensing, see \cite{Fornasier2010}).
	
	
	We note that most of the LOD systems seems to have been created to first provide a fast access to point (spatial indexing), and then adapted to provide LOD.
	Using the PCS, we can separate the indexing part, and the LOD scheme. From this stems less design constraints, more possibilities, and a method than is not dedicated to only one application (like visualisation). 
	

\subsection{Contribution}

	This work re-uses and combines existing and well established methods with a focus on simplicity and efficiency. As such, all the methods are tested on billions scale point cloud, and are Open Source for sake of reproducibility test and improvements
	  
	  
	\begin{itemize}
			\item   In (Section \ref{lod.method.order}) is to store the LOD implicitly in the ordering of the points rather than externally, avoiding any data duplication.
			Thus, we don't duplicate information, and the more we read points, the more precise of an approximation of the point cloud we get. If we read all the points, we have the original point cloud.
			
			\item  We introduce (MidOc, Section \ref{lod.method:midoc}), a simple way to order points in order to have an increasingly better geometric approximation of the point cloud when following this order.
			
			\item 	 In  (Section \ref{lod.method.dimdescriptor}) we show that this ordering embed information about the dimensionality of the sensed object,
			to the point of being a simple multi-scale dimensionality descriptor.
			We demonstrate the interest of this descriptor by comparing it to a state of the art dimensionality descriptor, then by assessing it potential by performing a Random Forest classification that can then be used for very fast pre-filtering of points, and other applications.
	\end{itemize}	
	
	 
\subsection{Plan}
	This work follows a classical plan of Introduction Method Result Discussion Conclusion (IMRAD).
	Section~\ref{lod.sec:method} presents the LOD solution, how it produces a dimensionality descriptor, and how this can leveraged for classification.  
	Section~\ref{lod.sec:result} reports on the experiments validating the methods.
	Finally, the details, the limitations, and potential applications are discussed in Section~\ref{lod.sec:discussion}.
	
