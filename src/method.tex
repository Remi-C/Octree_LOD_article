%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, RÃ©mi Cura
%% 
%% This file present the method of the article
%% ---------------------------------------------------------------------


\section{Method}
	\label{sec:method}
	
	In this section, we first present the Point Cloud Server (section \ref{method.PCS})(PCS \cite{cura2015})
	upon which this article is based. Then we introduce the LOD solution that we propose (\ref{method.intro})
	, which consists of reordering groups of points from less to more details (\ref{method.order}), and then choose which LOD is needed.
	Although any ordering can be used, we propose a simple geometric one (\ref{method:midoc}) which is fast and robust to density variation. 
	Furthermore, constructing this ordering produces a rough dimensionality descriptor (\ref{method.dimdescriptor}). 
	This descriptor can be used in the PCS to perform classification at the patch level (\ref{method.classif}). This classification can be directly transferred to points, or indirectly exploited in a pre-filtering step.
	
	\subsection{the Point Cloud Server}
	\label{method.PCS}
		\myimage{./illustrations/PCS/PCS}{Overall and storage organisations of the Point Cloud Server.}{fig:PCS}
		
		This article strongly depends on using the Point Cloud Server described in \cite{cura2015},
		therefore we introduce its principle and key relevant features (see figure \ref{fig:PCS}).
		
		The PCS is a complete and efficient point cloud management system based on a database server that works on groups of points rather than individual points.
		This system is specifically designed to solve all the needs of point cloud users:
		fast loading, compressed storage, powerful filtering, easy data access and exporting, and integrated processing.
		
		The core of the PCS is to store groups of points (called patches) that are multi-indexed (spatially, on attributes, etc.), and represented with different generalisation depending on the applications.
		Points can be grouped with any rules.
		In this article, the points are regrouped spatially by cubes 1 \metre (Paris) or 50 \metre (Vosges) wide.
		
		All the methods described in this article are applied on patches.
		So what we propose is to reorder each patch following the MidOc ordering, allowing LOD and producing a dimensionality descriptor per patch. It can then be used to classify patches.
		
		We stress that our method used on any point cloud will provide LOD,
		but that using it with the PCS is much more interesting,
		and adds key feature such as spatial indexing, fast filtering, etc. 
	 
%	\subsection{Introduction}
%	\label{method.intro}
%	
%		creating a lod -> order points
%		by product as dimensionnality descriptor
%		classification
%		leveraging classification for pre-filtering
%	 
%	 	In this section we introduce a simple method which proposes free geometrical LOD features, at the price of a small preprocessing time.
%	 	The method relies on ordering the points so that reading the points following this order is going to gradually increase the details of the point cloud.
%	 	Many ordering could be used, we propose one 
%	 	We use a by-product of this method to perform efficient training and filtering with Random Forest. 
%	 	A patch is technically a subset of a point cloud, so also a point cloud. For generality we will use the term point cloud to describe both patch or a global point cloud for the rest of this article, as our method can be used indifferently on patches and on the global point cloud.
%	 	\\
%	 	Lastly, computing this LOD gives interesting by-product that can be used as a crude local dimensionality descriptor. We use that to perform extremely fast rough-filtering of massive point cloud. 
%	 	
%	 	
%	 	This method has been designed to work with Lidar data sets, but may be used on noisy Structure from Motion (SfM) point clouds after filtering.  
%	 	 
	 	 
	\subsection{Exploiting the order of points}
		\label{method.order}
		%\subsubsection{Principle} 
			
			\myimage{./illustrations/LOD/short_illustration_concept_lod/concept_Level_Of_Detail.png}{3 geometrical Level Of Detail (LOD) for the same point cloud. Starting from very low detail by reading the first point of the patch, reading further points increases the details.}{fig:lod-principle}
			
			We propose to exploit the ordering of points to indirectly store LOD information.
			Indeed, whatever the format, be it file or database based, points ends up as a list, which is ordered.
			
			The idea is then to exploit the order of this list, 
			that when reading the points from the beginning to the end, we get gradually a more accurate geometrical approximation of the point cloud (see figure \ref{fig:lod-principle}). 
			 
			For instance, for a given list $L$ of points ordered from $1$ to $N$.
			Reading the points $1$ to $5$ is going to give a rough approximation of the point cloud, and reading another $16$ points (points $1$ to $21$) is going to give a slightly better approximation. Reading points $1$ to $N$ is going to get the exact point cloud, so there is no data loss, nor data duplication.
		 
		%\subsubsection{Advantages}
			Leveraging the point ordering as LOD gives 3 main advantages.
			%\paragraph{No on-line processing time}
				Except a pre-processing step to write the point cloud following a given ordering, each time the user wants to get a Level Of Detail version of the point cloud, there is no computing at all (only data reading).
				This may not make a big difference for non-frequent reading, but in a context where the same point cloud is going to get used several times at several levels and by several users simultaneously (for instance Point Cloud as a Service), no processing time makes a big difference.
				%(See figure \ref{fig:all_lod_illustration} for an example with LOD visualisation.) 		
			
			%\paragraph{No data duplication}
				Another big advantage is that exploiting point ordering does not necessitate additional storage.
				This is an advantage on low level. It saves disk space (no data duplication, no index file). Because the LOD information is embedded right within the point cloud, it is perfectly concurrent-proof, i.e. the point cloud and the LOD can not become out of sync.
				(Even in heavy concurrent read/Write, a user would always get a coherent LOD).
				Lastly because the LOD only relies on ordering the original points, and does not introduces any other points or data, it avoids all precision-related issues that may come from aggregating.
			
			%\paragraph{Portable}
				The last advantage comes from the simplicity of using the ordering. 
				Because it is already something that all point cloud tools can deal with (a list of points!), it is portable. Most softwares do not change the points order inside a cloud (See Section \ref{result.os_softwares}).
				Even if a tool were to change the order, it is easy to add the ordering number as an attribute, which increases slightly the storage requirement, but is totally portable and can be used with all existing tools.
				This simplicity also implies that adapting tools to use this ordering is very easy.
	
	
	\subsection{ MidOc : an ordering for gradual geometrical approximation}
		\label{method:midoc}
		\subsubsection{Requirements and hypothesis}
		
		The method exploits the order of points to store LOD information, so that the more points are read, the more details we have on the point cloud.
		Obviously an ordering method that class the points from first LOD to last LOD is needed.
		So this ordering is in fact a measure of point relevance, that is how well a point represents the point cloud (in a neighbourhood depending of the LOD).
		
		This ordering will be used by with different point cloud and for many applications, and so can not be tailored to one.
		As such, we can only consider the geometry (the minimal constituent of a point).
		Because of the possible varying-density of the point cloud, the ordering method also have to recreate a regular-ish sampling.
		
		Although many ordering could be used (for example, a simple uniform-random ordering),
		a suitable one would have low-discrepancy (that is be well homogeneous in space), not be sensitive to density variations, be regular, be fast to compute and be deterministic.
		
		We make two hypothesis that are mostly verified on Lidar point cloud. The first hypothesis (disposable density)
		is that the density does not gives information about the nature of the object being sensed. 
		that is, depending on the sensing situation, some parts of the cloud are more or less dense, but this has nothing to do with the nature of the object sensed, thus can be discarded.
		The second hypothesis (low noise) is that the geometrical noise is low. We need this hypothesis because we can not rely on the variation of density to lessen the influence of outliers.
		
		A common method in LOD is to recursively divide a point cloud into groups and use the barycentre of the group as the point representing this group. The ground of this method is that the barycentre minimise the sum of squarred distance to the points, and as such is the optimal point if the point probability are normal distributions.
		
		However such method is extremely sensible to density variation, and creates new points. 
		
		\subsubsection{Introducing the MidOc ordering}
		
		\myimage{./illustrations/octree_ordering/octree_ordering_legend}{MidOc explained in 2D. Given a point cloud (Blue) and quad tree cells (dashed grey), the chosen point (green ellipse) is the one closest to the centre (black point) of the cell.}{fig:midoc-principle}
		
		We propose to a re-use of well known and well proven existing methods that is the octree subsampling (for instance, the octree subsampling is used in \cite{Girardeau-Montaut2014}.
		An octree is built over a point cloud, then for each cell of the octree the LOD point is the barycentre of the points in the cell.  With this, browsing the octree breadth-first provides the points of the different levels.
		
		We adapt this to cope with density variation, and to avoid creating new point by aggregating.   
		We name this ordering MidOc (middle of octree subsampling) for clarity of this article, nonetheless we are probably not the first to use it.
		
		The principle is very simple, and necessitate an octree over the point cloud.
		We illustrate it on Figure \ref{fig:midoc-principle} (in 2D for graphical comfort).
		We walk the octree breadth-first.
		For each non-empty cell, the point closest to the cell centre is chosen and assigned the cell level,
		and removed from the point possible to pick.
		The process can be reached before having chosen all possible points,
		in which case the remaining points are added to the list, with the level $L_\infty$.
		
		The result is a set of points with level $(P,L_i)$.
		Inside one level $L_i$, points can be ordered following various strategies (see Section \ref{method.intralevel}).
		
	
		  
		\subsubsection{Intra-level ordering}
		\label{method.intralevel}
		
		\myimage{./illustrations/intralevel_ordering/intralevel_ordering_combined}{Several possible ordering with varying coverage.}{fig:intralevel_ordering}
		
		The intra-level ordering will have an impact if the LOD is used in a continuous way,
		and moreover may influence methods that relies on low-discrepancy.
		Lets take the example where the goal is to find the plan that best fits a set of points
		and the method to do so in online (for instance it could be based on online robust PCA like in (\cite{Feng2013})).
		The plan fitting method reads the points of a designated level $L_i$, and compute online the best plan.
		See Figure \ref{fig:intralevel_ordering}
		If the point  
		The points are ordered either randomly or deterministically (for instance, with an inverted Z Morton curve (read backward) , ).
		
				
		\todoall{ajouter illustrations et explication de l'importance du choix de l'ordonnancement inter level
				penser que Linfiny est aussi un niveau}
		
		\subsubsection{Applications}
			We see 3 type of application for this ordering :
			\begin{itemize}
				\item graphical LOD : service for point cloud visualisation.
				\item density correction : service for complex processing that may fail to deal with important density variation
				\item point cloud generalisation, service for processing that may use only a fraction of points.
			\end{itemize}	
			
			We stress that the LOD are in fact almost continous (as in the third illustrations of \ref{fig:banner_image}).
			\myimage{./illustrations/Objects/Objects_assembled.jpg}{All successive levels for common objects (car, window, 2 wheelers, light, tree, people, pole, ground), color is intensity for other points.}{fig:lod-common-objects}
					
					
		\subsubsection{Implementation}
			
			This method has the same complexity as an octree construction. Similar strategies can be followed, pending on available resources.
			
			 
			The most straightforward implementation is streamed :
			The closest point to the center is stored in each octree cell.
			At the beginning the octree is empty. When receiving a point, traverse the octree update/creating cells and updating the closest point for each cell traversed.
			When the tree is computed, traverse it breadth-first to collect chosen points.
			The worst complexity is O(N*T), the entire octree must be stored in memory.
			
			The simplest implementation use a recursive strategy. it only necessitate a method that given a cell and a list of points chose the point closest to the center of the cell, then split the cell and the list of points for the next level, and recursively calls itself on this subcells with the sublists.
			
			We propose this kind of implementation in Python with extensive use of bit-level operations (cf \ref{subsubsec:bit_coordinates}) as proof of concept.
			The octree is not stored in memory, but it may prove difficult to parallelize (which is done at the data level anyway for our experiments). 
			
			
			
		\subsubsection{Efficiency and performance}
			\label{subsubsec:bit_coordinates}
			Octree construction may be avoided by simply reading coordinates bitwise in a correctly centred/scaled point cloud.
			We centre a point cloud so that the lowest point of all dimension is $(0,0,0)$, and scale it so that the biggest dimension is in $[0,1[$.
			The point cloud is then quantized into $[0..2**L-1]$ for each coordinate.
			The coordinates are now integers, and for each point, reading its coordinates bitwise left to right gives the position of the point in the octree for level of the bit read.
			This means performing that this centring/scaling/quantization directly gives the octree. Moreover, further operations can be performed using bit arithmetic, which is extremely fast.
			
			\paragraph{Example} 
				 \myimage{./illustrations/LOD/principle_of_binary_coordinate.png}{Principle of binary coordinates for a centered, scaled and quantized point cloud.}{fig:binary_coordinates_example}
				 
				 On this illustration the point $P$ has coordinates $(5,2)$ in a $[0,2^3-1]^2$ system. Reading the coordinates as binary gives $(b'101',b'010')$.
				 Thus we now that on the first level of a quad tree, $P$ will be in the right (x=$b'1xx'$) bottom (y=$b'0yy'$) cell.
				 For the next level, we divide the previous cell in 2, and read the next binary coordinate. $P$ will be in the left (x=$b'x0x'$) up (y=$b'y1y'$) cell. There is no computing required, only bit mask and data reading.
			
		\subsubsection{Advantages}	
		
			\todoall{si barycentre proche du centre du l'octree (ce qui arrive en moyenne), le point choisi est celui qui minimise lerreur parmi les points du nuage de points.}
			\paragraph{Common, Simple and efficient}
				This method feels classical and is based on Octree. This makes it simple to implement, and possibly extremely memory and CPU efficient.

				The complexity is $O(n_points*Levels)$ or less. It is very simple to implement.				
				Octree construction has been commonly done on GPU for more than a decade.
				
			\paragraph{Fixed density}
				This method can be used to guarantee an almost constant density for a given levels, even when the acquisition process produced varying-density point cloud.
				Thus using the output of this method is a safeguard for most of the complex point cloud processing methods that may be badly affected by extrem variation in density ( real world case illustrated in \href{fig:density-variation}{this illustration}).
			\paragraph{Generic}
				Few hypothesis are made on the points properties. In particular, this method works well with 2.5D point cloud (aerial single echo Lidar) and full 3D point cloud (urban 3D cloud with multi echo).
			\paragraph{Construction Descriptors}
				Lastly, the information given by this ordering can be used as a  geometrical descriptor of the point cloud.
				The number of points per level for each level gives a crude dimensionality descriptor of the geometrical nature (at the patch scale) for the object contained by the point cloud.
				
	\subsection{Using the ordering by-product as a crude dimensionality descriptor}
	
	
		\label{method.dimdescriptor}
		\subsubsection{Principle}
			During the ordering process the number of chosen point per each level can be stored.
			This number of chosen points per level gives an indication on the geometric nature of the object in the point cloud. 
			We demonstrate the use of this crude descriptor along with other simple feature with a Random Forest classifier on a real world dataset publicly available. 
			This patch classification can be used in other ways than pure classification.
		
		\subsubsection{Conceptual example}
			
			In the \href{fig:lod-common-objects}{figure \ref{fig:lod-common-objects}}, we manually segmented typical parts of a street in the Paris dataset: a car, a wall with window, a 2 wheelers, a public light, a tree, a person, poles and piece of ground including curbs.
			\\
			Due to the geometry of acquisition and sampling, the public light is almost a 3D line, resulting in the occupation of very few octree cells.
			A typical number of points chosen per level  for the public light would then be $(1,2,4,8)$, which looks like a $2^L$ function.
			A piece of ground is often extremely flat and very similar to a plan, which means that the points chosen per level could be $(1,4,16,64)$, a $4^L$ function.
			Lastly a piece of tree foliage is going to be very volumetric in nature, due to the fact that a leaf is about the same size as point spacing and is partly transparent to laser (leading to several echo).
			Then a foliage patch would typically be $(1,8,64,512)$ (if enough points), so a $8^L$ function.
	
	 
			
		\subsubsection{Patch classifier aspplications}
		
			The base method is to use Random Forest to classify patches (and not points directly).
			
			Because patches may contains points belonging to several classes, transferring the patch classes to points naturally increases the errors.
			
			We can forsee three type of application for patch classification.
			
			\paragraph{Speeding a complex point classifier}
				The first application could be to speed up and/or improve a complex per point classifier.
				For a speed up, the patch classifier could perform a first basic classification extremely fast, thus eliminating a large number of points, before the complex classifier is used. 
				If necessary, it is possible to artificially increase recall at the cost of a diminution of precision, like in \ref{fig:recall-increase}.
				 
				
			\paragraph{Improving a complex point classifier}
				Patch classifier can also be used to improve result of a complex classifier by performing a first rough analysis which may determinate which complex classifier to use amongst several, like Cascaded classifier.
				For instance a patch classified as urban object would lead to chose a classifier specialized in urban object, and not the general classifier. This is especially precious for classes that are statistically very minoritary.
				If necessary it is possible to artificially increase precision at the cost of recall.
				
				
				
			\paragraph{Filtering}
				Another application is filtering for applications that only require one class. When the learning is done, classifying is extremely fast.
				
				Many applications only need one class, and do not require all the points in it, but only a subset with good confidence.
				For this it is possible to artificially boost the precision by accepting only high confidence prediction.
				For instance computing a Digital Terrain Model (DTM) only requires ground points. Morevover, the ground will have many parts missing due to objects, so using only a part of all the points will suffice anyway. The patch classifier allow to find the ground patch extremely fast.
				Another example is registration. A registration process typically require reliable points to perform mapping and registration. In this case there is no need to use all points, and the patch classification can provide patches from ground and facade with high accuracy (for point cloud to point cloud or point cloud to 3D model registration) , or patches of objects and trees (for points cloud to landmark registration).
				In other applications, finding only a part of the points may be sufficient, for instance when computing a building map from faÃ§ade patches.
				  
			
		\subsubsection{Descriptors}  
			\paragraph{Crude dimensionality descriptor}
				Again we work at the patch level ($1^3$ or $50^3$ \cubic \meter).
				We order all patches following the MidOc ordering. For each ordered patch, we associate the number of points per level that where chosen.
				A Random Forest classifier is trained using the number of chosen points per level.
				We use the number of points for the level $[1..4]$ included. For each level, the number of points is normalized by the maximum number of points possible ($8^i$), so every feature is in $[0,1]$.
			
			\paragraph{Other simple features}
				We also use other very simple features that require almost no computing. Feature usage is then analysed afterwards.
				For the sake of simplicity and efficiency, all the feature use basic statistics per patch that need to be precomputed anyway by the storage compression mechanism. This free statistics are min, max, and average of any attribute.
				\\
				Contextual features are avoided. They are more in the spirit of complex classification, would require computing, and could introduce a bias (in our favor) in the result.
				However using the context after the classification is a lever to significantly improve recall.
		
		\subsubsection{Analyzing data set classes}
			\paragraph{Analysing class hierarchy} 
				The Paris data set classes are organized in a hierarchy (100 classes). 
				Because of the hierarchy and the unbalancing of classes, we first determinate or similar the classes are for the simple descriptors. This information is extracted from the confusion matrix, which is used as an affinity matrix. The matrix is clustered by spectral clustering and the result are interpreted as a graph of classes.
				From this we choose the classes to use.
				
			\paragraph{Balancing the data set} 
				We tried two classical strategies to balance the data set regarding the number of observation per class.
				The first is undersampling : we randomly undersample the observations to get roughly the same number of observation in every class.
				
				The second strategy is to compute a statistical weight for every observation based on the class prevalance. 
				This weight is then used in the learning process.
				
		
		\subsubsection{Patch classifier}
 
				
				To ensure significant results we follow a K-fold cross-validation method. 
				We randomly split the observations into K parts, then for each part, we use the K-1 others to learn and predict on the part.
				All the evaluation are then performed on the total of predicted observations.
				
				
				Contrary to classical classification method, we are not only interested in precision and recall per class, but also by the evolution of precision when prediction confidence varies.
				
				In fact, for a filtering application, we can leverage the confidence information provided by the Random Forest method to artificially boost precision (at the cost of recall diminution). We can do this by limiting the minimal confidence allowed for every prediction.
				Similarly, it is possible for some classes to increase recall at the cost of precision by using the result of a first patch classification and then incorporate in the result the other neighbour patches. 
			 	
			 	We stress that if the goal is to detect objects (and not classify each point), this strategy can be extremely efficient.
			 	For instance if we are looking for objects that are big enough to be in several patches (e.g. a car).
			 	In this case we can perform the classification (which is very fast and efficient), then keep only highly confident predictions, and then use the position of predictions to perform a local search for car limits.
			 	The classical alternative solution would be to perform a per point classification on each point, which would be extremely slow.
			 	
			 	
		\subsubsection{Advantages}
			\paragraph{Dimensionality descriptor}
			\begin{itemize}			
				\item simple : 
					Dimensionality feature for point clouds are already well researched, and can be more precisely computed (\cite{Demantke2014}), with less sensibility to outliers (but more to density variation). However This kind of feature is generally designed at the point level, and is more complex.
					Using the result of the MidOc ordering has the advantage of being free and extremely simple. 
				\item Efficient :
					Moreover, because $x_0 \rightarrow (2^0)^x$,
					 $x_1 \rightarrow (2^1)^x$, $x_2 \rightarrow (2^2)^x$ diverge very fast, we only need to use few levels to have a quite good descriptor. For instance, using $L=2$, we have $D=4$, $16$ or $64$ , which are very distinguishable values, and don't require a density above $70$ points \per patch. 
				\item Density and scale independent :
					As long as the patch contains a minimal number of points, the descriptors is density and scale invariant.
				\item Mixed result : 
					Lastly a mixed result (following neither of the $x_i \rightarrow (2^i)^x$ function) can be used as an indicator that the patch contains mixed geometry, either due to nature of the objects in the patch, or due to the way the patch is defined (sampling).
			\end{itemize}	
			
			\paragraph{Patch classification}
			\begin{itemize}
				\item simple and fast
					When the Random Forest classifier is trained, prediction is extremely fast.
				\item good result
					Even if the classifier works on patch that may contain points from several classes, the global results for well represented classes are not far from state of the art.
				\item many applications
					teh classification is not necessarly interesting per see, but also for fast filtering or other applications.
			\end{itemize}
			
		
	\subsection{Classification with the PCS}
		\label{method.classif}
		
