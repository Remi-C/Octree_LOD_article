%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, RÃ©mi Cura
%% 
%% This file present the method of the article
%% ---------------------------------------------------------------------


\section{Method}
	\label{sec:method}
	
	In this section, we first present the Point Cloud Server (section \ref{method.PCS})(PCS \cite{cura2015})
	upon which this article is based. Then we introduce the LOD solution that we propose (\ref{method.intro})
	, which consists of reordering groups of points from less to more details (\ref{method.order}), and then choose which LOD is needed.
	Although any ordering can be used, we propose a simple geometric one (\ref{method:midoc}) which is fast and robust to density variation. 
	Furthermore, constructing this ordering produces a rough dimensionality descriptor (\ref{method.dimdescriptor}). 
	This descriptor can be used in the PCS to perform classification at the patch level (\ref{method.classif}). This classification can be directly transferred to points, or indirectly exploited in a pre-filtering step.
	
	\subsection{the Point Cloud Server}
	\label{method.PCS}
		\myimage{./illustrations/PCS/PCS}{Overall and storage organisations of the Point Cloud Server.}{fig:PCS}
		
		This article strongly depends on using the Point Cloud Server described in \cite{cura2015},
		therefore we introduce its principle and key relevant features (see figure \ref{fig:PCS}).
		
		The PCS is a complete and efficient point cloud management system based on a database server that works on groups of points rather than individual points.
		This system is specifically designed to solve all the needs of point cloud users:
		fast loading, compressed storage, powerful filtering, easy data access and exporting, and integrated processing.
		
		The core of the PCS is to store groups of points (called patches) that are multi-indexed (spatially, on attributes, etc.), and represented with different generalisation depending on the applications.
		Points can be grouped with any rules.
		In this article, the points are regrouped spatially by cubes 1 \metre (Paris) or 50 \metre (Vosges) wide.
		
		All the methods described in this article are applied on patches.
		So what we propose is to reorder each patch following the MidOc ordering, allowing LOD and producing a dimensionality descriptor per patch. It can then be used to classify patches.
		
		We stress that our method used on any point cloud will provide LOD,
		but that using it with the PCS is much more interesting,
		and adds key feature such as spatial indexing, fast filtering, etc. 
	 
%	\subsection{Introduction}
%	\label{method.intro}
%	
%		creating a lod -> order points
%		by product as dimensionnality descriptor
%		classification
%		leveraging classification for pre-filtering
%	 
%	 	In this section we introduce a simple method which proposes free geometrical LOD features, at the price of a small preprocessing time.
%	 	The method relies on ordering the points so that reading the points following this order is going to gradually increase the details of the point cloud.
%	 	Many ordering could be used, we propose one 
%	 	We use a by-product of this method to perform efficient training and filtering with Random Forest. 
%	 	A patch is technically a subset of a point cloud, so also a point cloud. For generality we will use the term point cloud to describe both patch or a global point cloud for the rest of this article, as our method can be used indifferently on patches and on the global point cloud.
%	 	\\
%	 	Lastly, computing this LOD gives interesting by-product that can be used as a crude local dimensionality descriptor. We use that to perform extremely fast rough-filtering of massive point cloud. 
%	 	
%	 	
%	 	This method has been designed to work with Lidar data sets, but may be used on noisy Structure from Motion (SfM) point clouds after filtering.  
%	 	 
	 	 
	\subsection{Exploiting the order of points}
		\label{method.order}
		%\subsubsection{Principle} 
			
			\myimage{./illustrations/LOD/short_illustration_concept_lod/concept_Level_Of_Detail.png}{3 geometrical Level Of Detail (LOD) for the same point cloud. Starting from very low detail by reading the first point of the patch, reading further points increases the details.}{fig:lod-principle}
			
			We propose to exploit the ordering of points to indirectly store LOD information.
			Indeed, whatever the format, be it file or database based, points ends up as a list, which is ordered.
			
			The idea is then to exploit the order of this list, 
			that when reading the points from the beginning to the end, we get gradually a more accurate geometrical approximation of the point cloud (see figure \ref{fig:lod-principle}). 
			 
			For instance, for a given list $L$ of points ordered from $1$ to $N$.
			Reading the points $1$ to $5$ is going to give a rough approximation of the point cloud, and reading another $16$ points (points $1$ to $21$) is going to give a slightly better approximation. Reading points $1$ to $N$ is going to get the exact point cloud, so there is no data loss, nor data duplication.
		 
		%\subsubsection{Advantages}
			Leveraging the point ordering as LOD gives 3 main advantages.
			%\paragraph{No on-line processing time}
				Except a pre-processing step to write the point cloud following a given ordering, each time the user wants to get a Level Of Detail version of the point cloud, there is no computing at all (only data reading).
				This may not make a big difference for non-frequent reading, but in a context where the same point cloud is going to get used several times at several levels and by several users simultaneously (for instance Point Cloud as a Service), no processing time makes a big difference.
				%(See figure \ref{fig:all_lod_illustration} for an example with LOD visualisation.) 		
			
			%\paragraph{No data duplication}
				Another big advantage is that exploiting point ordering does not necessitate additional storage.
				This is an advantage on low level. It saves disk space (no data duplication, no index file). Because the LOD information is embedded right within the point cloud, it is perfectly concurrent-proof, i.e. the point cloud and the LOD can not become out of sync.
				(Even in heavy concurrent read/Write, a user would always get a coherent LOD).
				Lastly because the LOD only relies on ordering the original points, and does not introduces any other points or data, it avoids all precision-related issues that may come from aggregating.
			
			%\paragraph{Portable}
				The last advantage comes from the simplicity of using the ordering. 
				Because it is already something that all point cloud tools can deal with (a list of points!), it is portable. Most softwares do not change the points order inside a cloud (See Section \ref{result.os_softwares}).
				Even if a tool were to change the order, it is easy to add the ordering number as an attribute, which increases slightly the storage requirement, but is totally portable and can be used with all existing tools.
				This simplicity also implies that adapting tools to use this ordering is very easy.
	
	
	\subsection{ MidOc : an ordering for gradual geometrical approximation}
		\label{method:midoc}
		\subsubsection{Requirements and hypothesis}
		
		The method exploits the order of points to store LOD information, so that the more points are read, the more details we have on the point cloud.
		Obviously an ordering method that class the points from first LOD to last LOD is needed.
		So this ordering is in fact a measure of point relevance, that is how well a point represents the point cloud (in a neighbourhood depending of the LOD).
		
		This ordering will be used by with different point cloud and for many applications, and so can not be tailored to one.
		As such, we can only consider the geometry (the minimal constituent of a point).
		Because of the possible varying-density of the point cloud, the ordering method also have to recreate a regular-ish sampling.
		
		Although many ordering could be used (for example, a simple uniform-random ordering),
		a suitable one would have low-discrepancy (that is be well homogeneous in space), not be sensitive to density variations, be regular, be fast to compute and be deterministic.
		
		We make two hypothesis that are mostly verified on Lidar point cloud.
		The first hypothesis (disposable density) is that the density does not gives information about the nature of the object being sensed. 
		that is, depending on the sensing situation, some parts of the cloud are more or less dense, but this has nothing to do with the nature of the object sensed, thus can be discarded.
		The second hypothesis (low noise) is that the geometrical noise is low.
		We need this hypothesis because we can not rely on the variation of density to lessen the influence of outliers.
		
		A common method in LOD is to recursively divide a point cloud into groups and use the barycentre of the group as the point representing this group. The ground of this method is that the barycentre minimise the sum of squarred distance to the points, and as such is the optimal point if the point probability are normal distributions.
		
		However such method is extremely sensible to density variation, and artificially creates new points. 
		
		\subsubsection{Introducing the MidOc ordering}
		
		\myimage{./illustrations/octree_ordering/octree_ordering_legend}{MidOc explained in 2D. Given a point cloud (Blue) and quad tree cells (dashed grey), the chosen point (green ellipse) is the one closest to the centre (black point) of the cell.}{fig:midoc-principle}
		
		We propose to re-use of well known and well proven existing methods that is the octree subsampling (for instance, the octree subsampling is used in \cite{Girardeau-Montaut2014}.
		An octree is built over a point cloud, then for each cell of the octree the LOD point is the barycentre of the points in the cell.  With this, browsing the octree breadth-first provides the points of the different levels.
		
		We adapt this to cope with density variation, and to avoid creating new point by aggregating.   
		We name this ordering MidOc (middle of octree subsampling) for clarity of this article, nonetheless we are probably not the first to use it.
		
		The principle is very simple, and necessitate an octree over the point cloud (octree can be implicit though).
		We illustrate it on Figure \ref{fig:midoc-principle} (in 2D for graphical comfort).
		We walk the octree breadth-first.
		For each non-empty cell, the point closest to the cell centre is chosen and assigned the cell level,
		and removed from the available point to pick.
		The process can be stopped before having chosen all possible points,
		in which case the remaining points are added to the list, with the level $L_\infty$.
		
		The result is a set of points with level $(P,L_i)$.
		Inside one level $L_i$, points can be ordered following various strategies (see Section \ref{method.intralevel}).
		
		Because each point is assigned a level, we can store the total number of points per level (which produce a dimensionality descriptor, see Section \ref{method.dimdescriptor}).
		
		\subsubsection{Implementation}
		MidOc ordering is similar to octree building. Because Octree building has been widely researched, we test only two basic solutions.
		
		The first kind of implementation uses SQL queries. For each level, we compute the centres of the occupied cells using bit shifts and the closest point to these. Picked points are removed, and the process is repeated on the next level.
		It relies on the fact that knowing each point octree cell occupancy does not require to compute the octree (see Figure \ref{fig:binary_coordinates_example}).
		
		The second implementation uses python with a recursive strategy. it only necessitates a function that given a cell and a list of points chose the point closest to the centre of the cell, then split the cell and the list of points for the next level, and recursively calls itself on this subcells with the sublists.
		
		A more efficient and simpler implementation is possible by first ordering the points following the Morton curve, as  in \cite{Feng2014} (Section 2.5.1, page 37). 
		
		\subsubsection{Intra-level ordering}
		\label{method.intralevel}
		
		\myimage{./illustrations/intralevel_ordering/intralevel_ordering_combined}{Several possible orders with various coverage from bad to good. Revert Morton and Revert Hilbert have offset for illustration.}{fig:intralevel_ordering}
		
		The intra-level ordering will have an impact if the LOD is used in a continuous way,
		and moreover may influence methods that relies on low-discrepancy.
		More precisely, if only a part of the points in a level are going to be used,
		it may be essential that they cover well the spatial layout of the totality of points.
		
		Lets take the example where the goal is to find the plan that best fits a set of points
		and the method to do so in online (for instance it could be based on online robust PCA like in (\cite{Feng2013})).
		The plan fitting method reads one by one the points of a designated level $L_i$, and compute successively better plan estimation.
		
		The Figure \ref{fig:intralevel_ordering} presents some possible ordering. 
		If the plan detection method was applied on the Y ordering, it would necessitate a great number of points to compute a stable plan. For instance the first 16 points (1 column) would not permit to compute a plan.
		Similarly, if the point were ordered randomly, estimating a plan would still require lots of points, because uniform randomly chosen points are not well spread out (on the figure, the first 25 points are over represented in the upper left part).
		
		On the opposite, using a low discrepancy ordering like the Halton sequence makes the points well spread, while being quasi-random.
		Inverted space filling curves like the Morton or Hilbert curves also cover well space, at the price of being much more regulars.
		
		The Halton sequence ordering is obtained by generating a Halton sequence (nD points) and successively picking points closest to the Halton generated points.
		The revert Morton ordering and revert Hilbert ordering are the distance along Morton or Hilbert curve expressed in bit and read backward (and possibly offset).
		 
				
	\subsection{Crude dimensionality descriptor (MidOc by-product)} 
		\label{method.dimdescriptor}
		\subsubsection{Principle}
		When building the MidOc ordering process, the number of chosen points per level can be stored.
		Each ordered patch is then associated with a vector of number of points per level $D_c=(N_{L_{1}},..,N_{L_{\text{max}}})$.
		The number of picked point is directly the voxel occupancy for this level of the octree.
		In theory for a level $L$, a centred line would occupy $2^L$ cells, a centred plan $4^L$ cells, and a volume all of the cells ($8^L cells$).
		Thus, by comparing $D_c$ to theoretical $L_i  \rightarrow (2^i)^L,i \in [1..3]$ functions we retrieve information about the dimensionality of the patch (See Figure \ref{fig:dim_descriptor}). 
		\myimage{"./illustrations/dim_descriptor/dim_descriptor"}{Voxel occupancy is a crude dimensionality descriptor: 3D line, surface or volume occupy a different amount of voxels.}{fig:dim_descriptor}
		
		
		The Figure \ref{fig:lod-common-objects} illustrate this. Typical parts of a street in the Paris dataset were segmented: a car, a wall with window, a 2 wheelers, a public light, a tree, a person, poles and piece of ground including curbs.
		\\
		Due to the geometry of acquisition and sampling, the public light is almost a 3D line, resulting in the occupation of very few octree cells.
		A typical number of points chosen per level for a public light patch would then be $(1,2,4,8)$, which looks like a $2^L$ function.
		A piece of ground is often extremely flat and very similar to a planar surface,
		which means that the points chosen per level could be $(1,4,16,64)$, a $4^L$ function.
		Lastly a piece of tree foliage is going to be very volumetric in nature,
		due to the fact that a leaf is about the same size as point spacing and is partly transparent to laser (leading to several echo).
		Then a foliage patch would typically be $(1,8,64,512)$ (if enough points), so a $8^L$ function.
		
		\paragraph{Usage}

		Real Dimensionality feature for point clouds are already well researched, and can be more precisely computed (\cite{Demantke2014}), with less sensibility to outliers (but more to density variation). However This kind of feature is generally designed at the point level, and is more complex.
		Using the result of the MidOc ordering has the advantage of not necessitate extra computing and be extremely simple.
		
		Moreover, because $x_1 \rightarrow (2^1)^x$,
		$x_2 \rightarrow (2^2)^x$, $x_3 \rightarrow (2^3)^x$ diverge very fast,
		we only need to use few levels to have a quite good descriptor.
		For instance, using $L=2$, we have $D=4$, $16$ or $64$ , which are very distinguishable values, and don't require a total density above $70$ points \per patch.  
		As long as the patch contains a minimal number of points, the descriptors is density and scale invariant. 
		Lastly a mixed result (following neither of the $x_i \rightarrow (2^i)^x$ function) can be used as an indicator that the patch contains mixed geometry, either due to nature of the objects in the patch, or due to the way the patch is defined (sampling).
		
		Although it might be possible to go a step further and decompose a patch $D_c$ vector on the base of $x_i \rightarrow (2^i)^x, i \in [1..3]$, the direct and exact decomposition can't be used because the decomposition might depends on L. For instance a plane with a small line could appear as a plan for $L_1$ and $L_2$, and starts to appear differently over $L_3$ and higher level. In this case, an Expectation-Maximization scheme might be able to decompose robustly.

			 
		
		
	\subsection{Classification with the Point Cloud Server}
		\label{method.classif}
		
		\todoall{expliquer le schema global de la classification: 
				in base out of base
				but : validation dim, pas forcement classif
				descripteurs : ortos ou simple
				a cause du jeux de donnÃ©es complique, clustering de classes
				}
		The base method is to use Random Forest to classify patches (and not points directly).
		
		Because patches may contains points belonging to several classes, transferring the patch classes to points naturally increases the errors.
		
		We can forsee three type of application for patch classification.
		
		\paragraph{Speeding a complex point classifier}
		The first application could be to speed up and/or improve a complex per point classifier.
		For a speed up, the patch classifier could perform a first basic classification extremely fast, thus eliminating a large number of points, before the complex classifier is used. 
		If necessary, it is possible to artificially increase recall at the cost of a diminution of precision, like in \ref{fig:recall-increase}.
		
		
		\paragraph{Improving a complex point classifier}
		Patch classifier can also be used to improve result of a complex classifier by performing a first rough analysis which may determinate which complex classifier to use amongst several, like Cascaded classifier.
		For instance a patch classified as urban object would lead to chose a classifier specialized in urban object, and not the general classifier. This is especially precious for classes that are statistically very minoritary.
		If necessary it is possible to artificially increase precision at the cost of recall.
		
		
		
		\paragraph{Filtering}
		Another application is filtering for applications that only require one class. When the learning is done, classifying is extremely fast.
		
		Many applications only need one class, and do not require all the points in it, but only a subset with good confidence.
		For this it is possible to artificially boost the precision by accepting only high confidence prediction.
		For instance computing a Digital Terrain Model (DTM) only requires ground points. Morevover, the ground will have many parts missing due to objects, so using only a part of all the points will suffice anyway. The patch classifier allow to find the ground patch extremely fast.
		Another example is registration. A registration process typically require reliable points to perform mapping and registration. In this case there is no need to use all points, and the patch classification can provide patches from ground and facade with high accuracy (for point cloud to point cloud or point cloud to 3D model registration) , or patches of objects and trees (for points cloud to landmark registration).
		In other applications, finding only a part of the points may be sufficient, for instance when computing a building map from faÃ§ade patches.
		
		
		\subsubsection{Descriptors}  
		\paragraph{Crude dimensionality descriptor}
		Again we work at the patch level ($1^3$ or $50^3$ \cubic \meter).
		We order all patches following the MidOc ordering. For each ordered patch, we associate the number of points per level that where chosen.
		A Random Forest classifier is trained using the number of chosen points per level.
		We use the number of points for the level $[1..4]$ included. For each level, the number of points is normalized by the maximum number of points possible ($8^i$), so every feature is in $[0,1]$.
		
		\paragraph{Other simple features}
		We also use other very simple features that require almost no computing. Feature usage is then analysed afterwards.
		For the sake of simplicity and efficiency, all the feature use basic statistics per patch that need to be precomputed anyway by the storage compression mechanism. This free statistics are min, max, and average of any attribute.
		\\
		Contextual features are avoided. They are more in the spirit of complex classification, would require computing, and could introduce a bias (in our favor) in the result.
		However using the context after the classification is a lever to significantly improve recall.
		
		\subsubsection{Analyzing data set classes}
		\paragraph{Analysing class hierarchy} 
		The Paris data set classes are organized in a hierarchy (100 classes). 
		Because of the hierarchy and the unbalancing of classes, we first determinate or similar the classes are for the simple descriptors. This information is extracted from the confusion matrix, which is used as an affinity matrix. The matrix is clustered by spectral clustering and the result are interpreted as a graph of classes.
		From this we choose the classes to use.
		
		\paragraph{Balancing the data set} 
		We tried two classical strategies to balance the data set regarding the number of observation per class.
		The first is undersampling : we randomly undersample the observations to get roughly the same number of observation in every class.
		
		The second strategy is to compute a statistical weight for every observation based on the class prevalance. 
		This weight is then used in the learning process.
		
		
		\subsubsection{Patch classifier} 
		
		To ensure significant results we follow a K-fold cross-validation method. 
		We randomly split the observations into K parts, then for each part, we use the K-1 others to learn and predict on the part.
		All the evaluation are then performed on the total of predicted observations.
		
		
		Contrary to classical classification method, we are not only interested in precision and recall per class, but also by the evolution of precision when prediction confidence varies.
		
		In fact, for a filtering application, we can leverage the confidence information provided by the Random Forest method to artificially boost precision (at the cost of recall diminution). We can do this by limiting the minimal confidence allowed for every prediction.
		Similarly, it is possible for some classes to increase recall at the cost of precision by using the result of a first patch classification and then incorporate in the result the other neighbour patches. 
		
		We stress that if the goal is to detect objects (and not classify each point), this strategy can be extremely efficient.
		For instance if we are looking for objects that are big enough to be in several patches (e.g. a car).
		In this case we can perform the classification (which is very fast and efficient), then keep only highly confident predictions, and then use the position of predictions to perform a local search for car limits.
		The classical alternative solution would be to perform a per point classification on each point, which would be extremely slow.
		
		
		\subsubsection{Advantages}
		
		
		\paragraph{Patch classification}
		\begin{itemize}
			\item simple and fast
			When the Random Forest classifier is trained, prediction is extremely fast.
			\item good result
			Even if the classifier works on patch that may contain points from several classes, the global results for well represented classes are not far from state of the art.
			\item many applications
			the classification is not necessarly interesting per see, but also for fast filtering or other applications.
		\end{itemize}
