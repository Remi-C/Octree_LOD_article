%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, Rémi Cura
%% 
%% This file present the method of the article
%% ---------------------------------------------------------------------


\section{\label{sec:method}Method}
	\subsection{introduction}
	 	In this section we introduce a simple method which proposes free geometrical LOD features, at the price of a small preprocessing time.
	 	The method relies on ordering the points so that reading the point following this order is going to gradually increase details of the point cloud.
	 	We use a by-product of this method to perform efficient training and filtering with random forest.
	 	
	 	
	 	This method has been designed to work with Lidar data set, but may be used on noisy Structure from Motion (SfM) point clouds after filtering.
	 	This method has been used inside our point cloud data management system which is centered on a Point Cloud Server (the article is being written, see \todorewrite{put Point Cloud Server presentation}for a very detailed presentation of it).
	 	
	 	We stress that Point Cloud Server work with patches, , which are groups of points obtained by cutting the original point cloud into regular pieces (1 meter wide cube for Paris data set,50 meter wide cube for Vosges dataset ).
	 	A Patch is technically a subset of a point cloud, so also a point cloud. For generality we will use the term point cloud to describe both patch and global point cloud for the rest of this article, as our method can be used indifferently on patches and on the global point cloud.
	 	
	 	
	 	Lastly computing this LOD gives interesting by-product that can be used as crude local dimensionality descriptor. We use that to perform extremely fast rough-filtering of massive point cloud. 
	 	 
	\subsection{Exploiting The Order Of Points }
		\subsubsection{Principle} 
			
			\myimage{./illustrations/LOD/short_illustration_concept_lod/concept_Level_Of_Detail.png}{arg2}
			
			Starting from a patch (i.e. a piece of point cloud) and we generate geometrical levels of details on it.
			
			Point clouds are physically stored as a list of points (compressed or not). 
			
			Once the points are written to the disk, the information contained by a list of point is not the sum of information for each point. The big difference is that a list is ordered.
			
			This fact is common for every point cloud format, they are inherently ordered. In fact it would be quite hard to store the point in an unordered fashion (true random is expansive).
			
			The first contribution of this article is to propose to exploit this ordering to store information.
			The key idea is to order the points of the point cloud so that when reading the points from the beginning to the end, we get gradually a more accurate geometrical approximation of the point cloud. 
			
			We demonstrate it with an ordering presented in  \ref{subsec:midoc} , but we could use any ordering.
			
		\subsubsection{Conceptual Example}
			For a given list $L$ of points ordered from $1$ to $N$.
			Reading the points $1$ to $5$ is going to give a rough approximation of the point cloud, and reading another $16$ points (points $1$ to $21$) is going to give a slightly better approximation. Reading points $1$ to $N$ is going to get the exact point cloud, so there is no data loss, nor data duplication.
		
		\subsubsection{Advantages}
			This method has 3 majors advantages.
			\paragraph{No on-line processing time}
				Except a pre-processing step to write the point cloud following a given ordering, each time the user wants to get a level of detail version of the point cloud, there is no computing at all (only data reading).
				This may not make a big difference for non-frequent reading, but in a context where the same point cloud is going to get used several times at several levels and by several users simultaneously (for instance Point Cloud as a Service), no processing makes a big difference.
				(See \href{fig:all_lod_illustration}{this illustration} for an example with LOD visualisation.) 		
			\paragraph{No data duplication}
				Another big advantage of using the order of points to store the LOD information rather than using external structure data is that it is free regarding the storage.
				This is an advantage on low level. First it saves disk space, it ensure that all the information is at the same place, which avoids to perform OS level commands like going trough directories, opening the structure file, etc.
				Having no data duplication is also a security in concurrent use.
				In the scenario where a point cloud and the associated data structure are separated, and both are accessed by concurrent users, it is possible that one user change the point cloud. The associated data structure is then updated, but during this laps of time it is possible for another user to get updated point cloud and the wrong associated data structure.  
				On the opposite, when the ordering gives the LOD information, a user either get the old points with the old ordering, or the new points with the new ordering, which guarantee that data is in a coherent state.
				Lastly we avoid all precision-related issues raised by traditionnal subsample methods because the order doesn't change a bit of the existing points and their attributes, but only there order in the point cloud.
			\paragraph{Portable}
				The last advantage comes from the simplicity of using the ordering. 
				Because it is already something that all point cloud tools can deal with (a list of points!), this way to create LOD is portable. Most softwares do not change the points order inside a cloud.
				Even if a tool were to change the order, it is easy to add the ordering number as an attribute, which increases a little bit storage size, but is totally portable and can be used with all existing tools.
				This simplicity also implies that adapting tools to use this ordering is very easy.
	
	\subsection{ \label{subsec:midoc} MidOc : an ordering for gradual geometrical approximation}
		\subsubsection{principle}
			Ordering the points necessitate an efficient method, because using LOD is already a trade-off between data information loss and data size reduction.
			
			We note that having usable LOD is only possible because Lidar point clouds have intrinsic structure we must exploit. 
			Precisely because we do a trade-off, we must exploit the intrinsic structure of the point cloud.
			
			For this, we make some assumption that are mostly verified on Lidar point cloud  :			
			\begin{itemize}
			
				\item Point cloud represents 3D surfaces sensed by a fix or mobile sensor (with the exception of multi-echo, which is correctly dealt with anyway).
				\item geometrical noise (outliers) is low.
				\item the density may vary, but we don't want to preserve it, nor does it give information about the nature of the object being sampled.
				
				that is, depending on the sensing situation, we some parts of the cloud are more or less dense, but this has nothing to do with the nature of the object sensed, thus must not be preserved.
			\end{itemize}
			We rely on a classical middle of octree subsampling (called MidOc in this article for simplicity) to create an ordering. This is a re-use of well known and well proven existing methods (for instance, the octree subsampling is used in \todoref{put reference to CloudCompare}).
			We name this ordering for clarity of this article, nonetheless we don't think we are the first to use it.
			
			\myimage{./illustrations/octree_ordering.png}{\label{fig:midoc-principle}principle of MidOc explained on a Quad tree for comfort}
			
			Again the principle is very simple, and is to take for each level the points closest to centre of its octree voxel, if any.
			\begin{algorithm}
				\KwData{A list of $N$ points}
				\KwResult{the MidOc ordering of the points for an octree of $L$ depth } 
				
				\ForEach{$level$ in [0..L-1]}{
					\ForEach{$cell$ of $level$ that is non empty}{
						compute $center$ of the $cell$\;
						Choose the point in the $cell$ that is closest to the $center$, if it exists\;
					}
				}
				order chosen points by level (coarse to fine), random (or hash or inverted Z curve)\;
				\caption{MidOc principle}
			\end{algorithm} 
			 
			 We illustrate this principle on 
			\href{fig:midoc-principle}{this figure} in 2D (quad tree) for graphical comfort. The layer are level 0 to 2. For each layer we compute the closest point(large green point) to the center of each cell (medium black point) using the squared euclidian distance (continuous red tone). The points picked on a level are not available for further level (no duplication of information).
			When reaching the desired max level, all the remaining points are ordered either randomly or deterministically (for instance, with an inverted Z Morton curve (read backward) ).
			We use the same random or deterministic order for points picked on the same level.
			Overall, the ordering is then (tree depth ascending, random or inverted Morton).
			
		\subsubsection{Implementation}
			
			This method has the same complexity as an octree construction. Similar strategies can be followed, pending on available resources.
			
			 
			The most straightforward implementation is streamed :
			The closest point to the center is stored in each octree cell.
			At the beginning the octree is empty. When receiving a point, traverse the octree update/creating cells and updating the closest point for each cell traversed.
			When the tree is computed, traverse it breadth-first to collect chosen points.
			The worst complexity is O(N*T), the entire octree must be stored in memory.
			
			The simplest implementation use a recursive strategy. it only necessitate a method that given a cell and a list of points chose the point closest to the center of the cell, then split the cell and the list of points for the next level, and recursively calls itself on this subcells with the sublists.
			
			We propose this kind of implementation in Python with extensive use of bit-level operations (cf \ref{subsubsec:bit_coordinates}) as proof of concept.
			The octree is not stored in memory, but it may prove difficult to parallelize (which is done at the data level anyway for our experiments). 
			
		\subsubsection{\label{subsubsec:bit_coordinates}efficiency and performance}
			Octree construction may be avoided by simply reading coordinates bitwise in a correctly centred/scaled point cloud.
			We centre a point cloud so that the lowest point of all dimension is $(0,0,0)$, and scale it so that the biggest dimension is in $[0,1[$.
			The point cloud is then quantized into $[0..2**L-1]$ for each coordinate.
			The coordinates are now integers, and for each point, reading its coordinates bitwise left to right gives the position of the point in the octree for level of the bit read.
			This means performing that this centring/scaling/quantization directly gives the octree. Moreover, further operations can be performed using bit arithmetic, which is extremely fast.
			
			\paragraph{Example} 
				 \myimage{./illustrations/LOD/principle_of_binary_coordinate.png}{\label{fig:binary_coordinates_example}Principle of binary coordinates for a centered, scaled and quantized point cloud}
				 On this illustration the point $P$ has coordinates $(5,2)$ in a $[0,2^3-1]^2$ system. Reading the coordinates as binary gives $(b'101',b'010')$.
				 Thus we now that on the first level of a quad tree, $P$ will be in the right (x=$b'1xx'$) bottom (y=$b'0yy'$) cell.
				 For the next level, we divide the previous cell in 2, and read the next binary coordinate. $P$ will be in the left (x=$b'x0x'$) up (y=$b'y1y'$) cell. There is no computing required, only bit mask and data reading.
			
		\subsubsection{advantages}	
			\paragraph{Common, Simple and efficient}
				This method feels classical and is based on Octree. This makes it simple to implement, and possibly extremely memory and CPU efficient.

				The complexity is $O(n_points*Levels)$ or less. It is very simple to implement.				
				Octree construction has been commonly done on GPU for more than a decade.
				
			\paragraph{Fixed density}
				This method can be used to guarantee an almost constant density for a given levels, even when the acquisition process produced varying-density point cloud.
				Thus using the output of this method is a safeguard for most of the complex point cloud processing methods that may be badly affected by extrem variation in density ( real world case illustrated in \href{fig:density-variation}{this illustration}).
			\paragraph{Generic}
				Few hypothesis are made on the points properties. In particular, this method works well with 2.5D point cloud (aerial single echo Lidar) and full 3D point cloud (urban 3D cloud with multi echo).
			\paragraph{Construction Descriptors}
				Lastly, the information given by this ordering can be used as a  geometrical descriptor of the point cloud.
				The number of points per level for each level gives a crude dimensionality descriptor of the geometrical nature (at the patch scale) for the object contained by the point cloud.
				
	\subsection{Using the ordering by-product as a crude dimensionality descriptor}
		\subsubsection{Principle}
			During the ordering process the number of chosen point per each level can be stored.
			This number of chosen points per level gives an indication on the geometric nature of the object in the point cloud. 
			We demonstrate the use of this crude descriptor with a Random Forest classifier on a real world dataset publicly available.
			@\todorewrite{ cite Demantké thesis because he creates sophisticated dimensionality indicators}
			We don't try to provide a per point classification, but a classification per  patch. 
			The idea is that for many patches we may only use this simple classifier. 
			For instance a complex classifier could be speeded by first using the simple one, then use the complex one on parts that the simple classifier is not confident with.
			This type of descriptor could then be used for cascaded or hierarchical classification. 
			Another application is filtering. If the classification process is very fast, one can easily eliminate large parts of the point cloud when looking for a given type of points (for instance, looking for ground points, or tree points).
		\subsubsection{Conceptual example}
			 
			In the \href{fig:lod-common-objects}{figure \ref{fig:lod-common-objects}}, we manually segmented a typical street extracts in the Paris dataset: a car, a wall with window, a 2 whelers, a public light, a tree, a person, poles and piece of ground including curbs.
			\\
			Due to the geometry of acquisition and sampling, the public light is almost a 3D line, resulting in the occupation of very few octree cells.
			A typical number of points chosen per level  for the public light would then be $(1,2,4,8)$, which looks like a $2^L$ function.
			A piece of ground is often extremely flat and very similar to a plan, which means that the points chosen per level could be $(1,4,16,64)$, a $4^L$ function.
			Lastly a piece of tree foliage is going to be very volumetric in nature, due to the fact that a leaf is about the same size as point spacing and is partly transparent to laser (leading to several echo).
			Then a foliage patch would typically be $(1,8,64,512)$ (if enough points), so a $8^L$ function.
			
		\subsubsection{Method}
			\paragraph{Crude dimensionality descriptor}
				Again we work at the patch level ($1^3$ or $50^3$ \cubic \meter).
				We order all patches following the MidOc ordering. For each ordered patch, we associate the number of points per level that where chosen.
				A Random Forest classifier is trained using the number of chosen points per level.
				We use the number of points for the level $[1..4]$ included. For each level, the number of points is normalized by the maximum number of points possible ($8^i$), so every feature is in $[0,1]$.
			
			\paragraph{Other simple features}
				We also use other very simple features that are almost do not require computing. Features usage is then analysed afterwards.
				For the sake of simplicity and efficiency, all the feature use basic statistics per patch that need to be precomputed anyway by the storage compression mechanism. This free statistics are min, max, and average of any attribute.
				\\
				Contextual features are avoided. They are more in the spirit of complex classification, would require computing, and could introduce a bias (in our favor) in the result.
				However using the context after the classification is a lever to significantly improve recall.
			
			\paragraph{Choosing a class in class hierarchy}
				Both data sets have a class hierarchy. Because our goal is not to classify every classes, but efficiently filter patches, we first determinate which classes are separable using only this simple descriptors. 
				This allow to choose which level of the hierarchy can be used.
				
			\paragraph{Balancing data set}
			\todo{Proof reading stopped here! }
				We tried two classical strategies to balance the data set regarding the number of observation per class.
				The first is undersampling : we randomly undersample the observations to get roughly the same number of observation in every class.
				
				The second strategy is to compute a statistical weight for every observation based on the class prevalance. 
				We then use this to weight the learning.
				
				To ensure significant results we use a K-fold strategy. 
				We randomly split the observations into K parts, then for each part, we use the K-1 others to learn and predict on the part.
				
				All the evaluation are then performed on the total of predicted observations.
				
			\paragraph{classifying}
				
				The second step is to perform classification to evaluate its potential.
				Contrary to classical classification method, we are not only interested in precision and recall per class, but also by the evolution of precision when prediction confidence varies.
				
				In fact, for a filtering application, we can leverage the confidence information provided by the Random Forest method to artificially boost precision (at the cost of recall diminution). We can do this by limiting the minimal confidence allowed for every prediction.
			 	
			 	We stress that if the goal is to detect objects (and not classify each point), this strategy can be extremely efficient.
			 	For instance if we are looking for objects that are big enough to be in several patches (e.g. a car).
			 	In this case we can perform the classification (which is very fast and efficient), then keep only highly confident predictions, and then use the position of predictions to perform a local search for car limits.
			 	In this case the classical solution would be to perform a per point classification on each point.
			 	
			 	
		\subsubsection{advantages}
			\paragraph{simple}
				Dimensionality feature for point clouds are already well researched, and can be more precisely computed (\cite{Demantke2014}), with less sensibility to outliers (but more to density variation). However This kind of feature is generally designed at the point level, and is more complex.
				Using the result of the MidOc ordering has the advantage of being free and extremely simple. 
			\paragraph{Efficient}
				Moreover, because x, x**2, x**3 diverge very fast, we only need to use few levels to have a quite good descriptor. For instance, using L=2, we have D=4, 16 or 64 , which are very distinguishable values, and don't require a density above 70 points/patch. 
			\paragraph{Density and scale independent}
				As long as the patch contains a minimal number of points, the descriptors is density and scale invariant.
			\paragraph{Mixed result}
				Lastly a mixed result (following neither of the x**n function) can be used as an indicator that the patch contains mixed geometry, either due to nature of the objects in the patch, or due to the way the patch is defined (sampling).
				
			
			
		
		
		
