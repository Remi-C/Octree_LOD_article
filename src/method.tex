%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, RÃ©mi Cura
%% 
%% This file present the method of the article
%% ---------------------------------------------------------------------


\section{Method}
	\subsection{introduction}
	 	In this section we introduce a simple and new method which proposes free geometrical LOD features, at the price of few preprocessing time.
	 	This method has been designed to work with Lidar data set, and should be used on noisy SfM point clouds after filtering.
	 	Similarly, this method has been used inside our new point cloud data management system which is centred on a Point Cloud Server (the article is being written).
	 	At such, the example showcase a real urban scene where the point cloud has been cut into 1m3 points groups called patch.
	 	The method is then used on each patch.
	 	Lastly computing this LOD gives interesting by-product that can be used as crude classifiers, and allow to perform extremely fast rough-filtering of massive point cloud. 
	 	 
	\subsection{exploiting the order of points }
		\subsubsection{principle}
			We propose a very simple yet efficient method.
			We start from a point cloud and we want to generate geometrical levels of details on it.
			
			Point clouds are physically stored as a list of points. 
			Now, the information contained by a list of point is not the sum of information for each point.A big difference is that the list is ordered.
			
			This fact is common for every point cloud format. At one point, points must be written to disk, and they become ordered. In fact it would be quite hard to store the point in a totally unordered fashion (true random is expansive).
			The first contribution of this article is to propose to exploit this ordering to store information.
			The key idea is : we \bf{order} the points of the point cloud so that when reading the points from beginning to end, we get gradually a better and better geometrical approximation of the point cloud. We call this ordering "MidOc" ordering, but the idea to use order to store LOD works with any ordering.
			
		\subsubsection{conceptual example}
			So for instance if we have a list L of points ordered from 1 to n.
			Reading the points 1 to 5 is going to give a rough approximation of the point cloud. Reading another 16 (points 1 to 21) is going to give a slightly better approximation. Reading points 1 to n is going to get the exact point cloud, so there is no data loss, nor data duplication.
		
		\subsubsection{advantages}
			\paragraph{no on-line processing time}
				The advantages of this method is that except a pre-processing step to write the point cloud following the MidOc ordering, each time we want to get a level of detail version of the point cloud, there is no computing at all (only data reading).
				This may not make a big difference for non-frequent reading, but in a context where the same point cloud is going to get used several time at several level and by several users simultaneously, no processing makes a big difference.
				(See result section for an example with LOD visualisation.) 		
			\paragraph{no data duplication}
				Another big advantage of using the order of points to store the LOD information rather than using external structure data is that it is free storage wise.
				This is an advantage on low level. First we need less space on disk. Second we are sure that all the information is at the same place, which avoids to perform OS level commands like going trough directories, opening the structure file, etc.
				Having no data duplication is also a security in concurrent use.
				If somebody changes the point cloud, another user could use the old structure with the new points in the time where the new structure is computed.
				On the opposite, when the ordering gives the LOD information, a user either get the old points with the old ordering, or the new points with the new ordering, which guarantee that data is in a coherent state.
				Lastly by ordering rather than creating a sub sampled version, we avoid all precision-related issues, because the order doesn't change existing points and their attributes, but only there order in the point cloud.
			\paragraph{portable}
				The last advantage comes from the simplicity of using the ordering. 
				Because it is already something that all point cloud tools can deal with (a list of points!), this way to create LOD is portable. Most software don't change the points order inside a cloud.
				If a tool changes the order, it is easy to add the ordering number as an attribute, which increase a little bit storage, but is totally portable and can be used with all existing tools.
				This simplicity also implies that adapting tools to use this ordering is very easy.
	
	\subsection{MidOc : an ordering for gradual geometrical approximation}
		\subsubsection{principle}
			The key of LOD is efficiency, because using LOD is already a trade-off between data information loss and data size reduction.
			Thus we need an efficient method that order the points so that reading from the start toward the end is going to give a gradually better geometrical approximation of the point cloud.
			
			Precisely because we do a trade-off, we must exploit the intrinsic structure of the point cloud.
			For this, we make some assumption that are mostly verified on Lidar point cloud.
			- Point cloud represents mostly 3D surfaces sensed by a fix or mobile sensor (with the exception of multi-echo, which is correctly dealt with anyway).
			- geometrical noise (outliers) is low.
			- density may vary, but does not gives information (that is, depending on the sensing situation, we may have parts of the cloud that are more or less dense, but this has nothing to do with the nature of the object sensed, thus must not be preserved).
			- density of the LOD should be has constant has possible.
			
			For simplicity and speed we propose MidOc ordering, which is a re-use of well known existing methods.
			We name this ordering for clarity of this article, nonetheless we don't think we are the first to use it.
			Again the principle is very simple, and is to take for each level the points closest to centre of its octree voxel, if any.
			Given a point cloud of N points and a tree depth of T,
			We compute an octree of depth T over the bounding box of the point cloud.
			For each cell, we take the point closest to the centre of the cell if any.
			
			We re-unite the points chosen, avoiding duplicates by giving priorities to lowest (= bigger cell) level.
			Then we put together all the points, putting the chosen points first, ordered by lowest level,  and an inverse Z curve or random order. 
			The inverse Z curve is a Morton curve read backward bit wise and can be used if determinism is wanted, or more simply random order if determinism is not a constraint.
			We consider T has the maximum depth, thus we modify the algorithm to stop when a given level is not sufficiently filled to save computing.

			This method can have the same complexity as octree construction, because the only supplementary cost are to find the point closest to the middle , which can be done will constructing for free, and uniting the point at the end, which doesn't really need a sorting because we can use walk trough the octree width-first for this.
			
			Even a very crude approach, where we construct the octree, then use it to find the points closest to middle of cells, will be faster than O(n*ln(n)).
			
		\subsubsection{efficiency and performance}
			In fact we don't really need to construct the octree because octree can be obtained by reading coordinates bitwise in a correctly centred/scaled point cloud.
			For a given point cloud, if we centre it so that the lowest point is all dimension is (0,0,0), and scale it so the biggest dimension is in [0,1].
			Then we quantize this point cloud into [0,2**L-1].
			Coordinates are now integer, and for each points, reading the coordinates bitwise left to right gives the position of the point in octree for level 1-L.
			This means performing this centring/scaling/quantization directly gives the octree, and that further operation can be performed using bit arithmetic, thus be extremely fast.
			
			We also note that it is possible to propose a streamed implementation, thus enabling on the fly computing of it.
			We propose a python recursive implementation as a proof of concept.
			
			\paragraph{example} 
				For instance for a 2D point cloud with L = 3, and two points P1(4,3) and P2(5,2) already quantized in [0,7].
				Their binary form is P1(100,011) , P2(101,010).
				Reading the first bit (left-right order) of the points gives the coordinate of the cell in the level 1 of the octree.
				for P1 and P2 it is (1,0), which means the bottom right cell of the quad-tree
				The second bit gives the position inside the second level of the octree.
				It is (0,1), which means the upper left sub-cell of the previous pixel.
				The last bit is (0,1) for P1 and (1,0) for P2, which allows to tell they won't be in the same cell.
				@\todo{put a schema with pyramidal 4 level total quad tree and bitwise coordinates with accolades}
				 
			
		\subsubsection{conceptual example}
			give example with very high density in the middle (vehicle stopped), plus object, plus tree.
			
			
		\subsubsection{advantages}	
			\paragraph{Well known, Simple and efficient}
				This method is extremely classical and based on Octree This make it simple to implement, and possibly extremely memory and CPU efficient.

				The complexity is O(n*L) or less .				
				Octree construction can even be performed on GPU for extreme speed up.
				It is a matter of days to implement it.
			\paragraph{fixed density}
				This method guarantee a constant density for given levels, even when the acquisition process produced varying-density point cloud.
				Thus using the output of this method is a safeguard for most complex point cloud processing method that may be badly affected by singularity in density.
			\paragraph{permissive}
				This method is permissive because we make very few hypothesis on the points properties. In particular, this method works well with 2.5D point cloud (aerial single echo Lidar) and full 3D point cloud (urban 3D cloud with multi echo).
			\paragraph{construction descriptors}
				Lastly, we can leverage the information given by this ordering as a good geometrical descriptor of the point cloud.
				We can keep the number of points per level for each level, which can be a good descriptor of the geometrical nature of the object contained in the point cloud.
				
	\subsection{using the ordering by-product as a crude dimensionality descriptor}
		\subsubsection{principle}
			During the ordering of a point cloud, we can keep the number of chosen point for each level.
			Now this number of chosen points per level gives an indication on the geometric nature of the object in the point cloud. 
			We demonstrate the use of this extremely simple descriptor with a classical Random Forest classifier on a real world dataset publicly available.
			@\todorewrite{ cite DemantkÃ© thesis because he creates sophisticated dimensionality indicators}
			We don't try to provide a per point classification, but a classification per group of points (i.e patch) . 
			The idea is that for many points we may only use this simple classifier. So we could speed up a very complex classifier by first using the simple one, then use the complex one on parts that the simple classifier is not confident with.
			Another application is filtering. If the classification process is very fast, one can easily eliminate large parts of the point cloud when looking for a given type of points (for instance, looking for ground points).
		\subsubsection{conceptual example}
			@\todo{ put 3 images, pole, road, vegetation}
			For instance, we take 3 typical street extracts : a pole, a piece of ground, and a piece of tree.
			Due to geometry of acquisition and sampling, the pole is almost a 3D line, thus the points will be concentrated in very few octree cells.
			A typical number of points chosen per level would be (1,2,4,8), thus average a 2**L function.
			A piece of ground is often extremely flat and very similar to a plan (in  fact, more like a very large radius cylinder part).
			Thus the typical points chosen per level would be (1,4,16,64), a 4**L function.
			Lastly a piece of tree foliage is going to be very volumetric in nature, due to fact that leaf are about the same size as point spacing and are partly transparent for laser (leading to several echo).
			So a foliage patch would typically be (1,8,64,512) (if enough points), so a 8**L function.
			
		\subsubsection{method}	
		\subsubsection{advantages}
			\paragraph{simple}
				Dimensionality feature for point clouds are already well researched, and can be more precisely computed (\cite{Demantke2014}), with less sensibility to outliers. However This kind of feature is generally designed at the point level, and is more complex.
				Using the result of the MidOc ordering has the advantage of being free and simple. 
			\paragraph{Efficient}
				Moreover, because x, x**2, x**3 diverge very fast, we only need to use few levels to have a quite good descriptor. For instance, using L=2, we have D=4, 16 or 64 , which are very distinguishable values, and don't require a density above 70 points/patch. 
			\paragraph{Density and scale independent}
				As long as the patch contains a minimal number of points, the descriptors is density and scale invariant.
			\paragraph{Mixed result}
				Lastly a mixed result (following neither of the x**n function) can be used as an indicator that the patch contains mixed geometry, either due to nature of the objects in the patch, or due to the way the patch is defined (sampling).
				
			
			
		
		
		
