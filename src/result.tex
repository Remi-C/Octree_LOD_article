%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, RÃ©mi Cura
%% 
%% This file present the result of the article
%% ---------------------------------------------------------------------


 \section{ Result }
	 \label{lod.sec:result}
 	\subsection{Introduction to results}
 		We design and execute several experiments in order to validate all points  introduced in Section \ref{lod.sec:method}.
 		First we prove that is it effectively possible to leverage points order, even using canonical open sources software out of the box.
 		Second we perform MidOc ordering on very large point cloud and analyse the efficiency, quality and applications of the results.
 		Third we use the number of points chosen in the MidOc ordering as a descriptors for a random forest classifier on two large data sets, proving their usefulness.
 		Last we analyse the potential of this free descriptors, and what it brings when used in conjunction to other simple descriptors.
 		
		%\paragraph{Software stack}
 		The base DBMS is \cite{PostgreSQL2014}. The spatial layer \cite{PostGIS2014} is added to benefits from generic geometric types and multidimensional indexes. The specific point cloud storage and function come from \cite{pgPointCloud2014}. 
 		The MidOc is either plpgsql or made in python with \cite{SciPy2014}. 
 		The classification is done with \cite{scikit-image}, and the network clustering with \cite{Networkx2014}.
 		Timings are only orders of magnitude due to the influence of database caching.
 	
	%\subsection{Data set used}
		\myimageHL{./illustrations/chap2/histogram_of_density/paris_vosges_density_histogramm}{ Histogram of number of points per patch, with a logarithmic scale for X and Y axis}{lod.fig:hist-density-dataset}{0.5}
	 
 		We use two data sets. There were chosen as different as possible to further evaluate how proposed methods can generalise on different data (See Figure fig:hist-density-dataset for histogram of patch density ). 
 		%\subsubsection{IQmulus data set}
 		The first data set is \cite{IQmulus2014} (Paris data set), an open source urban data set with varying density, singularities, and very challenging point cloud geometry. 
 		Every point is labelled with a hierarchy of 100 classes.
 		The training set is only 12 millions points.
 		Only 22 classes are represented. We group points in $1 \cubic \metre$ cubes.
 		The histogram of density seems to follow an exponential law (See figure \ref{lod.fig:hist-density-dataset}), the effect being that many patches with few points exist. 
 		
 		%\subsubsection{Vosges data set}
 		We also use the Vosges data set, which is a very wide spread, aerial Lidar, 5.5 Billions point cloud. 
 		Density is much more constant at 10k pts/patch .
 		A vector ground truth about surface occupation nature (type of forest) is produced by the French Forest Agency. Again the classes are hierarchical, with 28 classes.
 		We group points in 50 $\times 50 \meter$ squares.
 		

	\subsection{Using the Point Cloud Server for experiments}
		%\subsubsection{Principle}
		%\paragraph{Server}
		All the experiments are performed using a Point Cloud Server (cf \cite{Cura2014}).
		The key idea are that point clouds are stored inside a DBMS (postgres), as patch. Patch are compressed groups of points along with some basic statistics about points in the group.
		We hypothesize that in typical point cloud processing workflow, a point is never needed alone, but almost always with its surrounding points.
	
		%\paragraph{Fast filtering}
		Each patch of points is then indexed in an R tree for most interesting attributes (obviously X,Y,Z but also time of acquisition, meta data, number of points, distance to source, etc.)
			
		Having such a meta-type with powerful indexes allows use to find points based on various criteria extremely fast. (order of magnitude : ms). 
		As an example, for a 2 Billion points dataset, we can find all patches in few milliseconds having : 
		 - between -1 and 3 meters high in reference to vehicle wheels
		 - in a given 2D area defined by any polygon 
		 - acquired between 8h and 8h10 - etc.
		 
		%\paragraph{Parallelism friendly}
		The PCS offers an easy mean to perform data-partition based parallelism. We extensively use it in our experiments. 


	\subsection{Exploiting the order of points}
		%\subsubsection{Experiment summary}
		\label{lod.result.os_softwares}
		We proposed to implicitly store LOD in the order of the points (Section \ref{lod.method.order}).
		In this first experiment we check that point cloud ordering is correctly preserved by common open source point cloud processing software.
		For this, we use a real point cloud, which we order by MidOc ordering. 
		We export it as a text file as the reference file.
		For each software, we read the reference file and convert it into another format, then check that the conversion did not change the order of points. 
		The tree common open source software tested are CloudCompare\footnote{\url{www.danielgm.net/cc}}
		, LasTools\footnote{\url{www.cs.unc.edu/~isenburg/lastools}} and Meshlab\footnote{\url{http://meshlab.sourceforge.net/}}.
		All pass the test.
			
	\subsection{MidOc: an ordering for gradual geometrical approximation}
		%\subsubsection{Experiment summary}
		\myimageHL{./illustrations/chap2/LOD/visual_result_left_right}{Schematic illustration of different LOD. Left to right, all points, then LOD 4 to 0. Visualized in cloud compare with ambient occlusion. Point size varies for better visual result.}{lod.fig:visual_LOD_left_right}{0.5}
		
		
		We first test the visual fitness of MidOc ordering.
		Then we compute MidOc for our two datasets and evaluate the trade-off between point cloud size and point cloud LOD. 
		As a proof of concept we stream a 3D point cloud with LOD to a browser.
		
		%\subsubsection{Visual evaluation}
		The figure \ref{lod.fig:visual_LOD_left_right} illustrates LOD on a typical street of Paris dataset
		The figure \ref{lod.fig:lod-common-objects} shows LOD on common street objects of various dimensionality.
			  
		%\subsubsection{Size versus LOD trade-off}
		We compute the size and canonical transfer time associated for a representative street point cloud.
		For this order of magnitude, the size is estimated at 5*4 Byte (5 floats) per point, and the (internet) transfer rate at 1 \mega $byte$\per \second.
		
		\begin{table}[ht]
			\centering
			\caption{Number of points per LOD for the point cloud in the Figure \ref{lod.fig:visual_LOD_left_right}
				, plus estimated transfer time at 1 \mega $byte$\per \second.}
			\label{lod.tab:lod-size-time}
			\scriptsize 
			\begin{tabular}{cccccc}
				\bf{Level} & \shortstack{\bf{Typical} \\ \bf{spacing (\centi \meter)}} & \shortstack{ \bf{Points} \\ \bf{number (k)}} & \shortstack{\bf{Percent of} \\ \bf{total size}} & \shortstack{\bf{Estimated} \\ \bf{time (\second)}}   \\
				\hline All & 0.2 to 5  & 1600 & 100 & 60 \\ 
				\hline 0 & 100 & 3 & 0.2 & 0.1 \\ 
				\hline 1 & 50 & 11.6 & 0.7 & 0.4 \\ 
				\hline 2 & 25 & 41 & 2.6 & 1.5 \\ 
				\hline 3 & 12 & 134 & 8.4 & 5 \\ 
				\hline 4 & 6 & 372 & 23 & 14 \\    
			\end{tabular} 
		\end{table}
			 
		%\subsubsection{Large scale computing}
		
		%\paragraph{MidOc implementation} 
		We use 3 implementations of MidOc, two being pure plpgsql (postgreSQL script langage), and one Python (See Section \ref{lod.method.midoc.implementation}).
		%\paragraph{Computing on very large dataset}
		We successively order all the Paris and Vosges data sets with MidOc, using 20 parallel workers, with a plpgsql implementation.
		The ordering is successful on all patches, even in very challenging areas where there are big singularities in density, and many outliers.
		The total speed is about 100 millions points/hour using in-base processing.
		We prototyped an out-of-base processing where the extraction of points from patch is done on the client, and reached a 180 \mega pts \per \hour.
		The same method, without any ordering (only converting patch to point then point to patch) reach a 2.3 B pts\per\hour.
		% with python patch reading writting, 10 process : 50k pts/sec
		%only data (no processing) : 650k pts/s
		We consider it to be at least 10 times too slow for practical use.
		We briefly analyse performances, and conclude that only 10 workers are efficient.
		
		 
		%\subsubsection{LOD stream}
		As a proof of concept we stream points from the data base to a browser \cite{IGN2014a}. Because patch may contain a large number of points and because the browser WebGL technology is limited in how much points it can display,
		we limit the number of points per patch sent to the browser using LOD. Patch are ordered with MidOc, so the visual artifact is greatly reduced, and the data loads more quickly, as expected. 


	\subsection{Excessive Density detection and correction} 
		\myimageHL{./illustrations/chap2/density/density_detection_and_correction}{Abnormal density detection and correction. Top points per patch (left) or density (right), green-yellow-red. Bottom reflectance in grey. }{lod.fig:density-correction}{0.5}
		
		%\paragraph{Density abnormality (peak)}
		We detect the abnormal density (explained in Section \ref{lod.method.density}) in the Paris data set in $\sim 100 \milli \second$ 
		(See Figure \ref{lod.fig:density-correction}). 
		In comparison, computing the density per point with neighbourhood is extremely slow (only for this 1.5 Million extract, 1 minute with CloudCompare,4x2.5GHz, 10cm rad) (top right illustration), and after the density is computed for each points, all the point cloud still need to be filtered to find abnormal density spot.
		
		%\paragraph{Simple correction}
		If the patch are ordered following MidOc, unneeded points are removed by simply putting a threshold on points per patch (bottom left, 1 to 5k points \per \cubic \meter , bottom right , 5k to 24 k pts \per \cubic \meter). It considerably reduces the number of points (-33\%). 
		
		This strategy can be automated by stating than no patch should return points over Level $L_i$. Then when getting points from the PCS, so that only points in those levels are sent.
	
	\subsection{Rough Dimensionality descriptor}
		\label{lod.result.dim_descriptor}
		We test the dimensionality descriptor ($ppl$) in two ways.
		First we compare the extracted ($Dim_{LOD}$) to the classical structure tensor based descriptor ($Dim_{cov}$).
		Second we assess how useful it is for classification, 
		by analysing how well it separates classes, and how much it is used when 
		several other features are available.
		
		\subsubsection{Comparing LOD-based descriptor with Structure tensor-based descriptor}
		%\paragraph{Computing ($Dim_{cov}$)}
		We compute $Dim_{cov}$ following the indications of \cite{Weinmann2015} to get $p_{dim}->[0..1]^3$, i.e. the probability to belong to [1D,2D,3D].
		We convert this to $Dim_{cov}$ with $Dim_{cov}=\sum_{i=1}^{3}{i*p_{dim}[i]}$.
		
		Optionally, we test a filtering option so that the maximum distance in biggest two dimensions is more equivalent. However this approach fails to significantly improve results.
		
		%\paragraph{Computing ($Dim_{LOD}$)}
		We test several method to extract $Dim_{LOD}$ from $ppl$.
		The first method is to compute $ Dim_{LODs}[i] = log2(ppl[i])/i$,
		which gives the simple dimensionality indice for each level.
		The second method is the same but work on occupancy evolution, with
		$Dim_{LODd}[i] = log2(ppl[i]/ppl[i-1])$ (discarding $L_0$).
		In both case the result is a dimensionality indice between 0 and 3 for each Level.
		We use both indices to fusion the dimensionality across Levels (  working on $Dim_{LODA} = Dim_{LODs}\bigcup Dim_{LODd}$).
		The first method uses a RANSAC (\cite{SciPy2014} implementation of \cite{Choi2009}) 
		to find the best linear regression. The slope gives an idea of confidence (ideally, should be 0),
		and the value of the line at the middle of abscissa is an estimate of $Dim_{LOD}$.
		The second method robustly filters $Dim_{LODA}$ based on median distance to median value and
		average the inliers to estimate $Dim_{LOD}$.
		
		$Dim_{cov}$ and $Dim_{LOD}$ are computed with in-base and out-of-base processing, the latter 
		being executed in parallel (8 workers).
		For 10k patches, 12 \mega pts, retrieving data and writing result accounts for $48\second$, computing $Dim_{LOD}$ to $8\second$, $Dim_{cov}$ to $64\second$. Computing $ppl$ (which is multiscale) using a linear octree takes between $58$ $L_6$ and $85 \second$ $L_8$.
		 (
		\myimageHL{"./illustrations/chap2/comparing_dim_desc/success_case"}{$Dim_{LOD}$ and $Dim_{cov}$ are mostly comparable, except for few patches (5\%, coloured)}{lod.fig:success_case}{0.5}
		
		
		%\paragraph{Comparing $Dim_{cov}$ and $Dim_{LOD}$}
		Comparing $Dim_{cov}$ and $Dim_{LOD}$ is not straightforward because the implicit definition of dimension is very different in the two methods.
		We analyse the patches where $\lvert Dim_{LOD} -  Dim_{cov}\rvert <=0.5$. 
		0.5 is an arbitrary threshold,
		but we feel that it represents the point above which descriptors will predict unreconcilable dimensions.
		Those patches represent 93\% of the data set (0.96 \% of points), with a correlation of 0.80.
		Overall the proposed dimensions are similar for the majority of patch, especially for well filled 1D and 2D patches (See Fig. \ref{lod.fig:success_case}).
		
		
		
		\myimageHL{"./illustrations/chap2/comparing_dim_desc/explaining_failure_case"}{Representative patches for $\lvert Dim_{LOD}$-$Dim_{cov}>0.5\rvert$. Most differences are explained by $Dim_{cov}$ limitations (See \ref{lod.result.dim_failure}.)
		}{lod.fig:explaining_failure_case}{0.5}
		
		We analyse the 684 remaining patches to look for possible explanations of the difference in dimension (See Fig. \ref{lod.fig:explaining_failure_case}).
		
		We consider the following four main sources of limitations from $Dim_{cov}$.
		\begin{itemize}%[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
			\label{lod.result.dim_failure}
			\item Elongated patch.\\
				$Dim_{cov}$=1.42
				,$Dim_{LOD}$=1.92. 
				If the patch is not roughly a square,$Dim_{cov}$ gives a bad estimation as it is biased by the un-symmetry of point distribution.
			\item Ellipsoid too simple.\\
				$Dim_{cov}$=1.68,
				$Dim_{LOD}$=2.24.
				$Dim_{cov}$ fits an ellipsoid, which can not cope with complex objects, especially when the barycentre does not happen to be at a favourable place. 
			\item Coping with heterogeneous sampling.\\
				$p_{dim}$=[0.56,0.32,0.12],
				$Dim_{cov}$=1.57,
				$Dim_{LOD}$=2.16.\\
				$Dim_{cov}$ is sensitive to difference in point density. The points on the bottom plan are much 3 times less dense than in the vertical plan, leading to a wrong estimate.
			\item Definition of dimension different.\\
				General:$Dim_{cov}\in[1.2,2.6],Dim_{LOD}\in[1.7..2.7]$\\
				This patch: $p_{dim}$=[0.11, 0.23, 0.66]\\
				$ppl$=[1,8,36,74..],
				$Dim_{LODD}$=[3.0,2.17,1.04].
				Trees are a good example of how the two descriptors rely on a different dimension definition. For $Dim_{cov}$ points may be well spread out, so usually $p_{3D}$ is high.
				Yet, tree patches are also subject to density variation, and may also be elongate, which renders $Dim_{cov}$ very variable.
				On the opposite, $Dim_{LOD}$ considers the dimensionality at different scale (See Fig. \ref{lod.fig:analysing_tree}). From afar a tree-patch is volumetric, at lower scal, it seems planar (leaf and small sticks form rough plans together). Lastly at small scale, the tree looks linear (sticks). 
		\end{itemize}
		 
  
		\subsubsection{Usefulness of rough descriptor for classification}
		\myimageHL{./illustrations/chap2/classif/class_clustering/only_lod_feature_2}{Spectral clustering of confusion matrix of Paris data set classification using only $ppl$ descriptor. Edge width and colour are proportional to affinity. Node position is determined fully automatically. Red-ish arrows are manually placed as visual clues.}{lod.fig:ppl-separator-power}{0.5}
		
		Using only the $ppl$ descriptor, a classification is performed on Paris data set, then a confusion matrix is computed.
		We use the spectral layout (see Section \ref{lod.method.classification.spectral_layout}) to automatically produce the graph in Figure \ref{lod.fig:ppl-separator-power}. 
		We manually ad 1D,2D and 3D arrows.
		On this graph, classes that are most similar according to the classification with $ppl$ are close. The graph clearly present an organisation following 3 axis. Those axis coincide with the dimensionality of the classes. For instance the "tree" classe as a strong 3D dimensionality. The "Punctual object" class, defined by "Objects which representation on a map should be a point", is strongly 1D (lines), with object like bollard and public light. The "Road" class is strongly 2D, the road being locally roughly a plan. The center of the graph is occupied by classes that may have mixed dimensionality, for instance "4+ wheeler" (i.e. car) may be a plan or more volumetric, because of the $1 \cubic \metre$ sampling.
		"Building" and "sidewalk" are not as clearly 2D as  the "road" class. Indeed, the patch of "sidewalk" class are strongly mixed (containing 22\mypercent of non-sidewalk points, See figure \ref{lod.fig:result-paris}). The building class is also not pure 2D because building facade in Paris contains balcony, building decoration and floors, whcih introduce lots of object-like patches, which explain that building is closer to the object cluster. (See Figure \ref{lod.fig:visual_LOD_left_right}, in the Level 3 for instance).
		The dimensionality descriptor clearly separates classes following their dimensionality, but can't separate classes with mixed dimensionality.
		
		
		To further evaluate the dimensionality descriptor, we introduce other classification features (see \ref{lod.result.classification}), perform classification and compute each feature importance.
		The overall precision and recall result of these classification is correct, and the $ppl$ descriptor is of significant use (See Figure \ref{lod.fig:result-paris} and \ref{lod.fig:result-vosges}), especially in the Vosges data set. The $ppl$ descriptor is less used in Paris data set, maybe because lots of classes can not really be defined geometrically, but more with the context.
		  
	\subsection{Patch Classification}
		\label{lod.result.classification}
		%\subsubsection{Experiment summary}
		 
		\subsubsection{Introducing other features}
		The dimensionality descriptor alone cannot be used to perform sophisticated classification,
		because many semantically different objects have similar dimension 
		(for instance, a piece of wall and of ground are dimensionally very similar
		, yet semantically very different).
		We introduce additional simple features for classification (See Section \ref{lod.method.classification.other_feature}). All use already stored patch statistics, and thus are extremely fast to compute.
		(P : for Paris , V : for Vosges: 
		- average of altitude regarding sensing device origin(P)
		- area of $patch\_bounding\_box$ (P) : 
		- patch height (P)
		- $points\_per\_level$ ($ppl$), level 1 to 4 (P+V)
		- average of intensity (P+V)
		- average of $number\_of\_echo$ (P+V) 
		- average Z (V)
		
		For Vosges data set, we reach a speed of 1 \mega points\per \second \per worker to extract those features.
		\subsubsection{Classification Setting} 
		Undersampling and weighting are used on the paris dataset. First Undersampling to reduce the over dominant building classe to a 100 factor of the smallest class support. Then weighting is used to compensate for differences in support. 
		For the Vosges data set only the weighting strategy is used. 
		The weighting approach is favoured over undersampling because it lessen variability of results when classes are very heterogeneous.
		
		To ensure significant results we follow a K-fold cross-validation method. We again compute a confusion matrix (i.e. affinity between classes) on the Paris data set to choose which level of class hierarchy should be used.
		fig:class-clustering-all-features
		
		
		\subsubsection{Analysing class hierarchy} 
		\myimageHL{./illustrations/chap2/classif/class_clustering/class_clustering_macro_all}{Result of automatic spectral clustering over confusion matrix for patch classification of Paris data set with all simple features. Edges width and colour are proportional to confusion. Manually drawn clusters for easier understanding.}{lod.fig:class-clustering-all-features}{0.5}
		
		Choosing which level of the class hierarchy to use depends on data set and applications.
		In a canonical classification perspective, we have to strongly reduce the number of classes if we want to have significant results.
		However reducing the number of class (i.e use a higher level in the classes hierarchy) also means that classes are more heterogeneous.
		  
		Both data set are extremely unbalanced (factor 100 or more). Thus our simple and direct Random Forest approach is ill suited for dealing with extremely small classes. (Cascading or one versus all framework would be needed).
		
		For Vosges data set a short analysis convince us to use 3 classes: Forest, Land, and other, on this three classes, the Land class is statistically overweighted by the 2 others.
		
		For the Paris data set, we again use a spectral layout to represent the affinity graph (See Figure \ref{lod.fig:class-clustering-all-features}).
		Introducing other features clearly helps to separate more classes.
		The graph also shows the limit of the classification, because some class cannot be properly defined without context (e.g. the side-walk, which is by definition the space between building and road, hence is defined contextually). 
		
		\subsubsection{Classification results}
		\myimageHL{./illustrations/chap2/classif/result_vosges/result_vosges_2}{Vosges dataset. (table 2) Precision(prec.), recall (rec.), support (supp.), and average percent of points of the class in the patches, for comparison with point based method (mix.). (table 1)Feature usage }{lod.fig:result-vosges}{0.5}
		We perform a analysis of error on Vosges dataset and we note that errors seem to be significantly correlated to distance to borders. 
				 
		\myimageHL{./illustrations/chap2/classif/result_paris/result_per_class_multilevel_paris_2}{Results for Paris data set: at various level of class hierarchy. Precision(prec.), recall (rec.), support (sup.) and average percent of points of the class in the patches of the class, for comparison with point based method (mix.). Classes of the same type are in the same continuous tone. Feature usage is evaluated for each level in the class hierarchy.}{lod.fig:result-paris}{0.5}
		The learning time is less than a minute, the predicting time is less than a second. 
		
		For both dataset, patches main contain points from several classes. We measure how much of the patch points pertain to the dominant class. The result is given in the columns "mix". For instance the patch of the class "building" contains an average of 98.6 \mypercent points of the class "building", whereas the patch from the class "forest" contains 88.3\mypercent points of the class "forest".
		Therefore, to provide a comparison with point based classification, we can compute the precision of the classification per point as $Precision_{point} = Precision_{patch} * Mix.$. (Same for recall).
		
		\subsubsection{Precision or Recall increase} 
		\myimageHL{./illustrations/chap2/precision_vs_recall/patch_classification_for_building}{Plotting of patches classified as building, using confidence to increase precision. Ground truth from IGN and Open Data Paris}{lod.fig:precision-increase}{0.5}
		
		%\paragraph{precision increase}
		As explained in Section \ref{lod.method.classification.using_confidence}, we can leverage the random forest confidence score to artificially increase the precision.
	
		We focus on the building class.
		As seen in the Figure \ref{lod.fig:precision-increase}, initial classification results (blue) are mostly correct.
		Yet, only keeping patches with high confidence may greatly increase precision (to $100$\mypercent).
		Further filtering on confidence can not increase precision, but will reduce the variability of the found building patches. 
		This result (red) would provides a much better base for building reconstruction for instance. 
				
		%\paragraph{Filtering: artificial increase of recall}
		\myimageHL{./illustrations/chap2/precision_vs_recall/ground_recall_increase}{Map of patch clustering result for ground. The classical result finds few extra patches that are not ground (blue), and misses some ground patches (red). Recall is increased by adding to the ground class all patches that are less than 2 meters in X,Y and 0.5 meter in Z around the found patches. Extra patches are much more numerous, but all the ground patches are found.}{lod.fig:recall-increase}{0.5}

		The patch classifier can also be used as a filtering preprocess.
		In this case, the goal is not to have a great precision, but to be fast and with a good recall.
		Such recall may be increased artificially for class of objects bigger than the sampling size ($1\cubic\metre$ for Paris).
		
		We take the example of ground classification (See Figure \ref{lod.fig:recall-increase}). 
		The goal is to find all ground patches very fast.
		We focus on a small area for illustration purpose. This area contains $3086$ patches, including $439$ ground patches.
		Patch classification finds $421$ ground patch, 
		with a recall of $92.7$\%.
		Using the found patch, all the surrounding patches (X,Y : $2$ \meter, Z : $0.5$ \meter ) are added to the result (few seconds).
		There are now $652$ patches in the result, and the recall is $100$\%.
		This means that from a filtering point of view, a complex classifier that would try to find ground points can be used on $652/3086=21\%$ of the data set, at the price of few seconds of computing, without any loss of information.
		
		
					 	  