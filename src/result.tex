%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, RÃ©mi Cura
%% 
%% This file present the result of the article
%% ---------------------------------------------------------------------


 \section{ Result }
	 \label{sec:result}
 	\subsection{Introduction to results}
 		We design and execute several experiments in order to validate all points that have been introduced in the Section \ref{sec:method}.
 		First we prove that is it effectively possible to leverage points order, even using canonical open sources software out of the box.
 		Second we perform MidOc ordering on very large point cloud and analyse the efficiency, quality and applications of the results.
 		Third we use the number of points chosen in the MidOc ordering as a descriptors for a random forest classifier on two large data sets, proving their usefulness.
 		Last we analyse the potential of this free descriptors, and what it brings when used in conjunction to other simple descriptors.
 		
		%\paragraph{Software stack}
 		The base DBMS is \cite{PostgreSQL2014}. The spatial layer \cite{PostGIS2014} is added to benefits from generic geometric types and multidimensional indexes. The specific point cloud storage and function come from \cite{pgPointCloud2014}. 
 		The MidOc is either plpgsql or made in python with \cite{SciPy2014}. 
 		The classification is done with \cite{scikit-image}, and the network clustering with \cite{Networkx2014}.
 		Timings are only orders of magnitude due to the influence of database caching.
 	
	%\subsection{Data set used}
		\myimage{./illustrations/histogram_of_density/paris_vosges_density_histogramm.png}{ Histogram of number of points per patch, with a logarithmic scale for X and Y axis}{fig:hist-density-dataset}
	 
 		We use two data sets. There were chosen as different as possible to further evaluate how proposed methods can generalise on different data (See Figure fig:hist-density-dataset for histogram of patch density ). 
 		%\subsubsection{IQmulus data set}
 		The first data set is \cite{IQmulus2014} (Paris data set), an open source urban data set with varying density, singularities, and very challenging point cloud geometry. 
 		Every point is labelled with a hierarchy of 100 classes.
 		The training set is only 12 millions points.
 		Only 22 classes are represented. We group points in $1 \cubic \metre$ cubes.
 		The histogram of density seems to follow an exponential law (See figure \ref{fig:hist-density-dataset}), the effect being that many patches with few points exist. 
 		
 		%\subsubsection{Vosges data set}
 		We also use the Vosges data set, which is a very wide spread, aerial Lidar, 5.5 Billions point cloud. 
 		Density is much more constant at 10k pts/patch .
 		A vector ground truth about surface occupation nature (type of forest) is produced by the French Forest Agency. Again the classes are hierarchical, with 28 classes. 
 		We group points in 50 $\times 50 \meter$ squares.
 		

	\subsection{Using the Point Cloud Server for experiments}
		%\subsubsection{Principle}
		%\paragraph{Server}
		All the experiments are performed using a Point Cloud Server (cf \cite{Cura2014}).
		The key idea are that point clouds are stored inside a DBMS (postgres), as patch. Patch are compressed groups of points along with some basic statistics about points in the group.
		We hypothesize that in typical point cloud processing workflow, a point is never needed alone, but almost always with its surrounding points.
	
		%\paragraph{Fast filtering}
		Each patch of points is then indexed in an R tree for most interesting attributes (obviously X,Y,Z but also time of acquisition, meta data, number of points, distance to source, etc.)
			
		Having such a meta-type with powerful indexes allows use to find points based on various criteria extremely fast. (order of magnitude : ms). 
		As an example we can find all points in a data set of 2 Billion points in a matter of milliseconds 
		 - between -1 and 3 meters high in reference to vehicle wheels
		 - in a given 2D area defined by any polygon 
		 - acquired between 8h and 8h10 - etc.
		 
		%\paragraph{Parallelism friendly}
		The PCS offers an easy mean to perform data-partition based parallelism. We extensively use it in our experiments. 


	\subsection{Exploiting the order of points}
		%\subsubsection{Experiment summary}
		\label{result.os_softwares}
		We proposed to implicitly store LOD in the order of the points (Section \ref{method.order}).
		In this first experiment we check that point cloud ordering is correctly preserved by common open source point cloud processing software.
		For this, we use a real point cloud, which we order by MidOc ordering. 
		We export it as a text file as the reference file.
		For each software, we read the reference file and convert it into another format, then check that the conversion did not change the order of points. 
		The tree common open source software tested are CloudCompare\footnote{\url{www.danielgm.net/cc}}
		, LasTools\footnote{\url{www.cs.unc.edu/~isenburg/lastools}} and Meshlab\footnote{\url{http://meshlab.sourceforge.net/}}.
		All pass the test.
			
	\subsection{MidOc: an ordering for gradual geometrical approximation}
		%\subsubsection{Experiment summary}
		\myimageFullPageWidth{./illustrations/LOD/visual_result_left_right}{Schematic illustration of different LOD. Left to right, all points, then LOD 4 to 0. Visualized in cloud compare with ambient occlusion. Point size varies for better visual result.}{fig:visual_LOD_left_right}
		
		
		We first test the visual fitness of MidOc ordering.
		Then we compute MidOc for both our dataset and evaluate the trade-off between point cloud size and point cloud LOD. 
		As a proof of concept we stream a 3D point cloud with LOD to a browser.
		
		%\subsubsection{Visual evaluation}
		The figure \ref{fig:visual_LOD_left_right} illustrates LOD on a typical street of Paris dataset
		The figure \ref{fig:lod-common-objects} shows LOD on common street objects of various dimensionality.
			  
		%\subsubsection{Size versus LOD trade-off}
		We compute the size and canonical transfer time associated for a representative street point cloud.
		For this order of magnitude, the size is estimated at 5*4 Byte (5 floats) per point, and the (internet) transfer rate at 1 \mega $byte$\per \second.
		
		\begin{table}[ht]
			\centering
			\caption{Number of points per LOD for the point cloud in the Figure \ref{fig:visual_LOD_left_right}
				, plus estimated transfer time at 1 \mega $byte$\per \second.}
			\label{tab:lod-size-time}
			\scriptsize 
			\begin{tabular}{cccccc}
				\bf{Level} & \shortstack{\bf{Typical} \\ \bf{spacing (\centi \meter)}} & \shortstack{ \bf{Points} \\ \bf{number (k)}} & \shortstack{\bf{Percent of} \\ \bf{total size}} & \shortstack{\bf{Estimated} \\ \bf{time (\second)}}   \\
				\hline All & 0.2 to 5  & 1600 & 100 & 60 \\ 
				\hline 0 & 100 & 3 & 0.2 & 0.1 \\ 
				\hline 1 & 50 & 11.6 & 0.7 & 0.4 \\ 
				\hline 2 & 25 & 41 & 2.6 & 1.5 \\ 
				\hline 3 & 12 & 134 & 8.4 & 5 \\ 
				\hline 4 & 6 & 372 & 23 & 14 \\    
			\end{tabular} 
		\end{table}
			 
		%\subsubsection{Large scale computing}
		
		%\paragraph{MidOc implementation} 
		We use 3 implementations of MidOc, two being pure plpgsql (postgreSQL script langage), and one Python (See Section \ref{method.midoc.implementation}).
		%\paragraph{Computing on very large dataset}
		We successively order all the Paris and Vosges data sets with MidOc, using 20 parallel workers, with a plpgsql implementation.
		The ordering is successful on all patches, even in very challenging areas where there are big singularities in density, and many outliers.
		The total speed is about 100 millions points/hour.
		We consider it to be at least 10 times too slow for practical use.
		We briefly analyse performances, and conclude that only 10 workers are efficient, and that most of the time is spent on getting the points and writing them back.
		
		 
		%\subsubsection{LOD stream}
		As a proof of concept we stream points from the data base to a browser \cite{IGN2014a}. Because patch may contain a large number of points and because the browser WebGL technology is limited in how much points it can display,
		we limit the number of points per patch sent to the browser. Patch are ordered with MidOc, so the visual artifact is greatly reduced, and the data loads more quickly, as expected. 


	\subsection{Excessive Density detection and correction} 
		\myimage{./illustrations/density/density_detection_and_correction}{Abnormal density detection and correction. Top points per patch (left) or density (right), green-yellow-red. Bottom reflectance in grey. }{fig:density-correction}
		
		%\paragraph{Density abnormality (peak)}
		We detect the abnormal density (explained in Section \ref{method.density}) in the Paris data set in $\sim 100 \milli \second$ 
		(See Figure \ref{fig:density-correction}). 
		In comparison, computing the density per point with neighbourhood is extremely slow (only for this 1.5 Million extract, 1 minute with CloudCompare,4x2.5GHz, 10cm rad) (top right illustration), and after the density is computed for each points, all the point cloud still need to be filtered to find abnormal density spot.
		
		%\paragraph{Simple correction}
		If the patch are ordered following MidOc, unneeded points are removed by simply putting a threshold on points per patch (bottom left, 1 to 5k points \per \cubic \meter , bottom right , 5k to 24 k pts \per \cubic \meter). It considerably reduces the number of points (-33\%). 
	
	\subsection{Dimensionality descriptor}
		\label{result.dim_descriptor}
		We test the dimensionality descriptor ($ppl$)by analysing how well it
		can be used to separate classes, and how much it is used when 
		several other features are available.
		
		%\subsubsection{Separator power of ppl descriptor for Paris data set}
		\myimage{./illustrations/classif/class_clustering/only_lod_feature_2.png}{Spectral clustering of confusion matrix of Paris data set classification using only $ppl$ descriptor. Edge width and colour are proportional to affinity. Node position is determined fully automatically. Red-ish arrows are manually placed to help understand}{fig:ppl-separator-power}
		
		Using only the $ppl$ descriptor, a classification is performed on paris data set, then a confusion matrix is computed.
		We use the spectral layout (see Section \ref{method.classification.spectral_layout}) to automatically produce the graph in Figure \ref{fig:ppl-separator-power}. 
		We manually ad 1D,2D and 3D arrows.
		On this graph, classes that are most similar according to the classification with $ppl$ are close. The graph clearly present an organisation following 3 axis. Those axis coincide with the dimensionality of the classes. For instance the "tree" classe as a strong 3D dimensionality. The "Punctual object" class, defined by "Objects which representation on a map should be a point", is strongly 1D (lines), with object like bollard and public light. The "Road" class is strongly 2D, the road being locally roughly a plan. The center of the graph is occupied by classes that may have mixed dimensionality, for instance "4+ wheeler" (i.e. car) may be a plan or more volumetric, because of the $1 \cubic \metre$ sampling.
		"Building" and "sidewalk" are not as clearly 2D as  the "road" class. Indeed, the patch of "sidewalk" class are strongly mixed (containing 22\mypercent of non-sidewalk points, See figure fig:result-paris). The building class is also not pure 2D because building facade in Paris contains balcony, building decoration and floors, whcih introduce lots of object-like patches, which explain that building is closer to the object cluster. (See Figure \ref{fig:visual_LOD_left_right}, in the Level 3 for instance).
		The dimensionality descriptor clearly separate classes following there dimensionality, but can't separate classes with mixed dimensionality.
		
		
		To further evaluate the dimensionality descriptor, we introduce other classification features (see \ref{result.classification}), perform classification and compute each feature importance.
		The overall precision and recall result of these classification is correct, and the $ppl$ descriptor is of significant use (See Figure \ref{fig:result-paris} and \ref{fig:result-vosges}), especially in the Vosges data set. The $ppl$ descriptor is less used in Paris data set, maybe because lots of classes can not really be defined geometrically, but more with the context.
		  
	\subsection{Patch Classification}
		\label{result.classification}
		%\subsubsection{Experiment summary}
		 
		\subsubsection{Introducing other features}
		The dimensionality descriptor alone cannot be used to perform sophisticated classification,
		because many semantically different objects have similar dimension 
		(for instance, a piece of wall and of ground are dimensionally very similar
		, yet semantically very different).
		We introduce additional simple features for classification (See Section \ref{method.classification.other_feature}). All use already stored patch statistics, and thus are extremely fast to compute.
		(P : for Paris , V : for Vosges: 
		- average of altitude regarding sensing device origin(P)
		- area of $patch\_bounding\_box$ (P) : 
		- patch height (P)
		- $points\_per\_level$ ($ppl$), level 1 to 4 (P+V)
		- average of intensity (P+V)
		- average of $number\_of\_echo$ (P+V) 
		- average Z (V)
		
		\subsubsection{Classification Setting} 
		Undersampling and weighting are used on the paris dataset. First Undersampling to reduce the over dominant building classe to a 100 factor of the smallest class support. Then weighting is used to compensate for differences in support. 
		For the Vosges data set only the weighting strategy is used. 
		The weighting approach is favoured over undersampling because it lessen variability of results when classes are very heterogeneous.
		
		To ensure significant results we follow a K-fold cross-validation method. We again compute a confusion matrix (i.e. affinity between classes) on the Paris data set to choose which level of class hierarchy should be used.
		fig:class-clustering-all-features
		
		
		\subsubsection{Analysing class hierarchy} 
		\myimage{./illustrations/classif/class_clustering/class_clustering_macro_all}{Result of automatic spectral clustering over confusion matrix for patch classification of Paris data set with all simple features. Edges width and colour are proportional to confusion. We manually draw clusters for easier understanding.}{fig:class-clustering-all-features}
		
		Choosing which level of the class hierarchy to use depends on data set and applications.
		In a canonical classification perspective, we have to strongly reduce the number of classes if we want to have significant results.
		However reducing the number of class (i.e use a higher level in the classes hierarchy) also means that classes are more heterogeneous.
		  
		Both data set are extremely unbalanced (factor 100 or more). Thus our simple and direct Random Forest approach is ill suited for dealing with extremely small classes. (Cascading or one versus all framework would be needed).
		
		For Vosges data set a short analysis convince us to use 3 classes: Forest, Land, and other, on this three classes, the Land class is statistically overweighted by the 2 others.
		
		For the Paris data set, we again use a spectral layout to represent the affinity graph (See Figure \ref{fig:class-clustering-all-features}).
		Introducing other features clearly helps to separate more classes.
		The graph also shows the limit of the classification, because some class cannot be properly defined without context (e.g. the side-walk, which is by definition the space between building and road, hence is defined contextually). 
		
		\subsubsection{Classification results}
		\myimage{./illustrations/classif/result_vosges/result_vosges_2.png}{Feature usage (first table).  Precision(prec.), recall (rec.), support (supp.), and average percent of points of the class in the patches, for comparison with point based method (mix.).}{fig:result-vosges}
		We perform a analysis of error on Vosges dataset and we remark that the error seems to be significantly correlated to distance to borders.
		The learning time is few minutes (monoprocess, python), the predicting time is few seconds (same).
				 
		\myimageFullPageWidth{./illustrations/classif/result_paris/result_per_class_multilevel_paris_2.pdf}{Results for Paris data set: at various level of class hierarchy. Precision(prec.), recall (rec.), support (sup.) and average percent of points of the class in the patches of the class, for comparison with point based method (mix.). Classes of the same type are in the same continuous tone. Feature usage is evaluated for each level in the class hierarchy.}{fig:result-paris} 
		The learning time is less than a minute, the predicting time is less than a second. 
		
		For both dataset, patches main contain points from several classes. We measure how much of the patch points pertain to the dominant class. The result is given in the columns "mix". For instance the patch of the class "building" contains an average of 98.6 \mypercent points of the class "building", whereas the patch from the class "forest" contains 88.3\mypercent points of the class "forest".
		Therefore, to provide a comparison with point based classification, we can compute the precision of the classification per point as $Precision_{point} = Precision_{patch} * Mix.$.
		
		\subsubsection{Precision or Recall increase} 
		
		%\paragraph{precision increase}
		As explained in Section \ref{method.classification.using_confidence}, we can leverage the random forest confidence score to artificially increase the precision.
		\myimage{./illustrations/precision_vs_recall/patch_classification_for_building.pdf}{Plotting of patches classified as building, using confidence to increase precision. Ground truth from IGN and Open Data Paris}{fig:precision-increase}.
		
		We focus on the building class.
		As seen in the Figure \ref{fig:precision-increase}, initial classification results (blue) are mostly correct.
		Yet, only keeping patches with high confidence may greatly increase precision (to $100$\mypercent).
		Further filtering on confidence can not increase precision, but will reduce the variability of the found building patches. 
		This result (red) provides a much better base for building reconstruction. 
				
		%\paragraph{Filtering: artificial increase of recall}
		\myimage{./illustrations/precision_vs_recall/ground_recall_increase.png}{Map of patch clustering result for ground. The classical result finds few extra patches that are not ground (blue), and misses some ground patches (red). Recall is increased by adding to the ground class all patches that are less than 2 meters in X,Y and 0.5 meter in Z around the found patches. Extra patches are much more numerous, but all the ground patches are found.}{fig:recall-increase}

		The patch classifier can also be used as a filtering preprocess.
		In this case, the goal is not to have a great precision, but to be fast and with a good recall.
		Such recall may be increased artificially for class of objects bigger than the sampling size ($1\cubic\metre$ for Paris).
		
		We take the example of ground classification (See Figure \ref{fig:recall-increase}). 
		The goal is to find all ground patches very fast.
		We focus on a small area for illustration purpose. This area contains $3086$ patches, including $439$ ground patches.
		Patch classification finds $421$ ground patch, 
		with a recall of $92.7$\%.
		Using the found patch, all the surrounding patches (X,Y : $2$ \meter, Z : $0.5$ \meter ) are added to the result (few seconds).
		There are now $652$ patches in the result, and the recall is $100$\%.
		This means that from a filtering point of view, a complex classifier that would try to find ground points can be used on $652/3086=21\%$ of the data set, at the price of few seconds of computing, without any loss of information.
		
		
					 	  