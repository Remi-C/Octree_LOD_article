%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, Rémi Cura
%% 
%% This file present the result of the article
%% ---------------------------------------------------------------------


 \section{Result}
 	\subsection{introduction to all experiments}
 		We design and execute several experiments in order to validate all points that have been introduced in the "method" part.
 		First we prove that is it effectively possible to leverage points order, even using out of the box most used open sources software
 		Second we perform MidOc ordering on very large point cloud and analyse the efficiency and quality of the results.
 		Third we use the number of points chosen in the MidOc ordering has a descriptors for a random forest classifier on two large data set.
 		We analyse the potential of this free descriptors, and what it brings when used in conjunction to other simple descriptors.
	\subsection{Point Cloud server introduction}
		All the experiments are performed using a Point Cloud Server (article is being written, but a very detailled presentation is accessible \todoref{put reference to postgres presentation}).
		The key idea are that point clouds are stored inside a DBMS (postgres), as patch. Patch are groups of points along with some basic statistics about points in the group.
		This organisation is based on the observation that in typical point cloud processing workflow, we never need a point alone, but more often a points and most likely its surrounding points.
		
		Having such a meta-type allows use to find points based on various criteria extremely fast. (order of magnitude : ms). 
		Cutting a point cloud into patches provides also a very easy parallel processing possibility, which we use in our experiments.
		
		For our experiments we cut terrestrial Lidar point cloud into 1m3 cubes oriented on (North ,Est,Z) axis.
		We cut aerial lidar point cloud into \todorewrite{put the cell size for Vosges dataset} m3.
		The choice of size is a compromise between speed, index size, patch size, typical feature size, etc.
		In fact the patch can be cut arbitrary, we chose this splitting for simplicity.
	\subsection{Data Set used}
		We use two data sets.
		First \todoref{cite IGQmulus data set}, an open source urban data set with varying density, singularities, and very challenging point cloud geometry.
		Data set is about 600Millions points, over 12 kms of road. Points are typically spaced by 5cm to 0.2 cm. It is a mutli echo laser (Riegl).
		We have access to a training set where every point is labeled. 
		Then \todoref{cite IGQmulus data set}, which is a very wide spread data set of 6 billions points. density is almost constant to \todo{insert avg density for Vosges}.
		We have access to a rough ground truth about surface occupation nature (type of forest).
		
	\subsection{Exploiting the order of points}
		\subsubsection{experiment summary}
			In this first experiment we check that point cloud ordering is correctly preserved by common open source point cloud processing software.
			For this, we use a real point cloud, to whoch we add MidOc ordering as rgb. 
			We export it as a ".txt" text file, this is the reference file.
			Then for each processing software, we read the reference file and convert it into another format, then check that the conversion didn't change the order. 
			The tree common open source software tested are CloudCompare, LasTools and Meshlab.
		\subsubsection{results}  
			All softwares passes test.
			Any software changing order of point-cloud can still be used if the order is exported as an attribute.
	\subsection{MidOc : an ordering for gradual geometrical approximation}
		\subsubsection{experiment summary}
			In this experiment we first test ordering on typical street objects,then on small subset of terrestrial and aerial Lidar to appreciate the fitness to use it for geometrical LOD.
			Then we compute MidOc for two big datasets and try to qualify result visually as well as identify computing bottleneck .
			Lastly we briefly test exploiting LOD to stream 3D point cloud with various resolution to a browser.
		
		\subsubsection{visual evaluation}
			Visual evaluation on typical objects (ground, façade, car, pole, vegetation)
			\paragraph{visual evaluation on a portion of street}
			
			\begin{table}[ht]
				\centering
				\caption{ number of points per LOD, plus estimated transfer time with modern internet connection}
				\scriptsize 
				\begin{tabular}{|c|c|c|c|c|c}
				\hline Level & \shortstack{Typical \\ spacing(cm)} & \shortstack{ points \\ number(k)} & \shortstack{percent of \\ total size} & \shortstack{estimated \\ time Internet(s)} & \shortstack{estimated \\ time LAN(s)} \\
				\hline All & 0.2 to 5  & 1600 & 100 & 60 & 3 \\ 
				\hline 0 & 100 & 3 & 0.2 & 0.1 & 0.005\\ 
				\hline 1 & 50 & 11.6 & 0.7 & 0.4 & 0.02\\ 
				\hline 2 & 25 & 41 & 2.6 & 1.5 & 0.075\\ 
				\hline 3 & 12 & 134 & 8.4 & 5 & 0.25\\ 
				\hline 4 & 6 & 372 & 23 & 14 & 0.7\\  
				
				\hline 
				\end{tabular} 
			\end{table}
			
			
			Visual results on a street (benchmark street, interesting parts)
			
			\todorewrite{Visual results on aerial (Vosges).}
			
		\subsubsection{large scale computing}
			We use 3 implementations of MidOc, two being pure plpgsql (postgres script langage), and one python.
			Computing MidOc on very large point-cloud (parallel)
			\todorewrite{compute on Vosges, 12 x parallel processing}
			Analysis -> Bottleneck is not computing , but read/update with new order. 
		\subsubsection{LOD stream}
			??(parler du stream de patch dans itowns?)??
			limitation = have to put the whole patch in memory, even when getting only few points.
			Conclusion : as is : interesting when bandwidth limited.  
	\subsection{using the ordering by-product as a crude dimensionality descriptor}
		\subsubsection{experiment summary}
			When computing MidOc ordering, we can easily store the number of points chosen per level.
			This experiment tries to evaluate the utility of such crude dimensionality descriptor using random forest as classifier.
			This dimensionality descriptor cannot be used to perform sophisticated classification, because many semantically different objects have similar dimension (for instance, a piece of wall and of ground are dimensionnaly very similar).
			We evaluate first what kind of reliable prediction can be made with the dimensionnality feature,
			then we add several very simple feature to provide a result to compare to.
			
			
		\subsubsection{data set}
			present IQmulus data set
			present Vosges data set
		\subsubsection{}
		
 