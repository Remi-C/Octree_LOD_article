%% ---------------------------------------------------------------------
%% Copyright 2014, Thales, IGN, RÃ©mi Cura
%% 
%% This file present the result of the article
%% ---------------------------------------------------------------------


 \section{ Result }
	 \label{sec:result}
 	\subsection{Introduction to results}
 		We design and execute several experiments in order to validate all points that have been introduced in the Section \ref{sec:method}.
 		First we prove that is it effectively possible to leverage points order, even using canonical open sources software out of the box.
 		Second we perform MidOc ordering on very large point cloud and analyse the efficiency, quality and applications of the results.
 		Third we use the number of points chosen in the MidOc ordering as a descriptors for a random forest classifier on two large data sets, proving their usefulness.
 		Last we analyse the potential of this free descriptors, and what it brings when used in conjunction to other simple descriptors.
 		
		%\paragraph{Software stack}
 		The base DBMS is \cite{PostgreSQL2014}. The spatial layer \cite{PostGIS2014} is added to benefits from generic geometric types and multidimensional indexes. The specific point cloud storage and function come from \cite{pgPointCloud2014}. 
 		The MidOc is either plpgsql or made in python with \cite{SciPy2014}. 
 		The classification is done with \cite{scikit-image}, and the network clustering with \cite{Networkx2014}.
 		Timings are only orders of magnitude due to the influence of database caching.
 	
	%\subsection{Data set used}
		\myimage{./illustrations/histogram_of_density/paris_vosges_density_histogramm.png}{ Histogram of number of points per patch, with a logarithmic scale for X and Y axis}{fig:hist-density-dataset}
	
	
 		We use two data sets. There were chosen as different as possible to further evaluate how proposed methods can generalise on different data (See Figure fig:hist-density-dataset for histogram of patch density ). 
 		%\subsubsection{IQmulus data set}
 		The first data set is \cite{IQmulus2014} (Paris data set), an open source urban data set with varying density, singularities, and very challenging point cloud geometry. 
 		Every point is labelled with a hierarchy of 100 classes.
 		The training set is only 12 millions points.
 		Only 22 classes are represented. We group points in $1 \cubic \metre$ cubes.
 		
 		%\subsubsection{Vosges data set}
 		We also use the Vosges data set, which is a very wide spread, aerial Lidar, 5.5 Billions point cloud. 
 		Density is much more constant at 10k pts/patch .
 		A vector ground truth about surface occupation nature (type of forest) is produced by the French Forest Agency.
 		We group points in 50 $\times 50 \meter$ squares.
 		

	\subsection{Using the Point Cloud Server for experiments}
		%\subsubsection{Principle}
		%\paragraph{Server}
		All the experiments are performed using a Point Cloud Server (cf \cite{Cura2014}).
		The key idea are that point clouds are stored inside a DBMS (postgres), as patch. Patch are compressed groups of points along with some basic statistics about points in the group.
		We hypothesize that in typical point cloud processing workflow, a point is never needed alone, but almost always with its surrounding points.
	
		%\paragraph{Fast filtering}
		Each patch of points is then indexed in an R tree for most interesting attributes (obviously X,Y,Z but also time of acquisition, meta data, number of points, distance to source, etc.)
			
		Having such a meta-type with powerful indexes allows use to find points based on various criteria extremely fast. (order of magnitude : ms). 
		As an example we can find all points in a data set of 2 Billion points in a matter of milliseconds 
		 - between -1 and 3 meters high in reference to vehicle wheels
		 - in a given 2D area defined by any polygon 
		 - acquired between 8h and 8h10 - etc.
		 
		%\paragraph{Parallelism friendly}
		The PCS offers an easy mean to perform data-partition based parallelism. We extensively use it in our experiments. 


	\subsection{Exploiting the order of points}
		%\subsubsection{Experiment summary}
		\label{result.os_softwares}
		We proposed to implicitly store LOD in the order of the points (Section \ref{method.order}).
		In this first experiment we check that point cloud ordering is correctly preserved by common open source point cloud processing software.
		For this, we use a real point cloud, which we order by MidOc ordering. 
		We export it as a text file as the reference file.
		For each software, we read the reference file and convert it into another format, then check that the conversion did not change the order of points. 
		The tree common open source software tested are CloudCompare\footnote{\url{www.danielgm.net/cc}}
		, LasTools\footnote{\url{www.cs.unc.edu/~isenburg/lastools}} and Meshlab\footnote{\url{http://meshlab.sourceforge.net/}}.
		All pass the test.
			
	\subsection{MidOc: an ordering for gradual geometrical approximation}
		%\subsubsection{Experiment summary}
		\myimageFullPageWidth{./illustrations/LOD/visual_result_left_right}{Schematic illustration of different LOD. Left to right, all points, then LOD 4 to 0. Visualized in cloud compare with ambient occlusion. Point size varies for better visual result.}{fig:visual_LOD_left_right}
		
		
		We first test the visual fitness of MidOc ordering.
		Then we compute MidOc for both our dataset and evaluate the trade-off between point cloud size and point cloud LOD. 
		As a proof of concept we stream 3D point cloud with various LOD to a browser.
		
		%\subsubsection{Visual evaluation}
		The figure \ref{fig:visual_LOD_left_right} illustrates LOD on a typical street of Paris dataset
		The figure \ref{fig:lod-common-objects} shows LOD on common street objects of various dimensionality.
			  
		%\subsubsection{Size versus LOD trade-off}
		We compute the size and canonical transfer time associated for a representative street point cloud.
		For this order of magnitude, the size is estimated at 5*4 Byte (5 floats) per point, and the (internet) transfer rate at 1 \mega $byte$\per \second.
		
		\begin{table}[ht]
			\centering
			\caption{Number of points per LOD for the point cloud in the Figure \ref{fig:visual_LOD_left_right}
				, plus estimated transfer time at 1 \mega $byte$\per \second.}
			\label{tab:lod-size-time}
			\scriptsize 
			\begin{tabular}{cccccc}
				\bf{Level} & \shortstack{\bf{Typical} \\ \bf{spacing (\centi \meter)}} & \shortstack{ \bf{Points} \\ \bf{number (k)}} & \shortstack{\bf{Percent of} \\ \bf{total size}} & \shortstack{\bf{Estimated} \\ \bf{time (\second)}}   \\
				\hline All & 0.2 to 5  & 1600 & 100 & 60 \\ 
				\hline 0 & 100 & 3 & 0.2 & 0.1 \\ 
				\hline 1 & 50 & 11.6 & 0.7 & 0.4 \\ 
				\hline 2 & 25 & 41 & 2.6 & 1.5 \\ 
				\hline 3 & 12 & 134 & 8.4 & 5 \\ 
				\hline 4 & 6 & 372 & 23 & 14 \\    
			\end{tabular} 
		\end{table}
			 
		%\subsubsection{Large scale computing}
		
		%\paragraph{MidOc implementation} 
		We use 3 implementations of MidOc, two being pure plpgsql (postgreSQL script langage), and one Python (See Section \ref{method.midoc.implementation}).
		%\paragraph{Computing on very large dataset}
		We successively order all the Paris and Vosges data sets with MidOc, using 20 parallel workers, with a plpgsql implementation.
		The ordering is successful on all patches, even in very challenging areas where there are big singularities in density, and many outliers.
		The total speed is about 100 millions points/hour.
		We consider it to be at least 10 times too slow for practical use.
		We briefly analyse performances, and conclude that only 10 workers are efficient, and that most of the time is spent on getting the points and writing them back.
		
		 
		%\subsubsection{LOD stream}
		As a proof of concept we stream points from the data base to a browser \cite{IGN2014a}. Because patch may contain a large number of points and because the browser WebGL technology is limited in how much points it can display,
		we limit the number of points per patch sent to the browser. Patch are ordered with MidOc, so the visual artifact is greatly reduced, and the data loads more quickly. 


	\subsection{Excessive Density detection and correction}
	
		\myimage{./illustrations/density/density_detection_and_correction}{Abnormal density detection and correction. Top points per patch (left) or density (right), green-yellow-red. Bottom reflectance in grey. }{fig:density-correction}
		
		%\paragraph{Density abnormality (peak)}
		We detect the abnormal density (explained in Section \ref{method.density}) in the Paris data set in $\sim 100 \milli \second$ 
		(See Figure \ref{fig:density-correction}). 
		In comparison, computing the density per point with neighbourhood is extremely slow (only for this 1.5 Million extract, 1 minute with CloudCompare,4x2.5GHz, 10cm rad) (top right illustration), and after the density is computed for each points, all the point cloud still need to be filtered to find abnormal density spot.
		
		%\paragraph{Simple correction}
		If the patch are ordered following MidOc, unneeded points are removed by simply putting a threshold on points per patch (bottom left, 1 to 5k points \per \cubic \meter , bottom right , 5k to 24 k pts \per \cubic \meter). It considerably reduces the number of points (-33\%).
			
	\subsection{Dimensionality descriptor and patch classification}
			To ensure significant results we follow a K-fold cross-validation method. 
			We randomly split the observations into K parts, then for each part, we use the K-1 others to learn and predict on the part.
			All evaluations are then performed on the total of predicted observations.
		\subsubsection{Experiment summary}
			For each patch, we store the associated number of points chosen per level ($ppl$) while computing MidOc ordering.  
			
			This dimensionality descriptor alone cannot be used to perform sophisticated classification, because many semantically different objects have similar dimension (for instance, a piece of wall and of ground are dimensionally very similar, yet semantically very different).
			An analysis of confusion matrix shows which meta classes are separatable or not.
			
			Extra descriptors are then needed :  (P : for Paris , V : for Vosges: 
		- $points\_per\_level$ ($ppl$), level 1 to 4 (P+V)
		- average of intensity (P+V)
		- average of $number\_of\_echo$ (P+V)
		- average of height regarding laser origin(P)
		- average Z (V)
		- patch height (P)
		- area of $patch\_bounding\_box$ (P) : 
		
		\subsubsection{Separator power of ppl descriptor for Paris data set}
			
			\myimage{./illustrations/classif/class_clustering/only_lod_feature_2.png}{Spectral clustering of confusion matrix of Paris data set classification using only $ppl$ descriptor. Edge width and colour are proportional to affinity. Node position is determined fully automatically. Red-ish arrows are manually placed to help understand}{fig:ppl-separator-power}
			
			Using only the $ppl$ descriptor, a classification is performed on paris data set, then a confusion matrix is computed.
			A spectral clustering of this matrix interpreted as a distance matrix between classes is performed. This clustering is used to place classes on the illustration.
			We manually ad 1D,2D and 3D arrows. 
		
		\subsubsection{Analyzing data set classes}
			\paragraph{Balancing the data set}  
				Undersampling and weighting are used on the paris dataset. First Undersampling to reduce the over dominant building classe to a 100 factor of the smallest class support. Then weighting is used to compensate for differences in support.
				
				For the Vosges data set only the weighting strategy is used.
				
				The weighting approach is favoured over undersampling because it lessen variability of results when classes are very heterogeneous.
				
				
			\paragraph{Analysing class hierarchy} 
		 
				Choosing which level of the class hierarchy uses depends on data set and applications.
				In a canonical classification perspective, we have to strongly reduce the number of classes if we want to have significant results.
				However reducing the number of class (i.e use a higher level in the classes hierarchy) also means that classes are more heterogeneous.
				
				Both data set are extremely unbalanced (factor 100 or more). Thus our simple and direct Random Forest approach is ill suited for dealing with extremely small classes. (Cascading or one versus all framework would be needed).
				
				For Vosges data set a short analysis convince us to use 3 classes: Forest, Land, and other, on this three classes, the Land class is statistically overweighted by the 2 others.	
				
				For the Paris data set, we analyse the confusion matrix by spectral clustering.
				
				\myimage{./illustrations/classif/class_clustering/class_clustering_macro_all}{Result of automatic spectral clustering over confusion matrix for patch classification of Paris data set with all simple features. Edges width and colour are proportional to confusion. We manually draw clusters for easier understanding.}{fig:class-clustering-all-features}
				
				
				This analysis is useful to show the limit of the classification, because some class cannot be properly defined without context (e.g. the side-walk, which is by definition the space between building and road, hence is defined contextually).
		
		\subsubsection{Patch classifier}
			\paragraph{Vosges data set}
		
				We perform a analysis of error on Vosges dataset and we remark that the error seems to be significantly correlated to distance ot borders.
				
				\myimage{./illustrations/classif/result_vosges/result_vosges_2.png}{Feature usage (first table).  Precision(prec.), recall (rec.), support (supp.), and average percent of points of the class in the patches, for comparison with point based method (mix.).}{fig:result-vosges}
				
				The learning time is few minutes (monoprocess, python), the predicting time is few seconds (same).
				
				
			\paragraph{Paris data set}
			
				\myimageFullPageWidth{./illustrations/classif/result_paris/result_per_class_multilevel_paris_2.pdf}{Results for Paris data set: at various level of class hierarchy. Precision(prec.), recall (rec.), support (sup.) and average percent of points of the class in the patches of the class, for comparison with point based method (mix.). Classes of the same type are in the same continuous tone. Feature usage is evaluated for each level in the class hierarchy.}{fig:result-paris}
		 
				The learning time is less than a minute, the predicting time is less than a second. 
			
		\subsubsection{Patch classifier applications} 
			
			\paragraph{Artificial increase of precision}
				We demonstrate the use of artificial increase of precision. Initial results (blue) are mostly correct, but by only keeping patches with high confidence, it is possible to increase precision to $100$\%. Above $100$\%, we reduce the variability of the found building patches. 
				\myimage{./illustrations/precision_vs_recall/patch_classification_for_building.pdf}{Plotting of patches classified as building, using confidence to increase precision. Ground truth from IGN and Open Data Paris}{fig:precision-increase}.
				
				In this example the precision was already very good (most of the blue patches are in building), but increasing precision to reduce class heterogeneity provides a much better base for building reconstruction.
				
				It is possible to use this method only when precision is  a rising function of confidence given by random forest.
				This is the case for 4+wheeler class.  
				\myimage{./illustrations/classif/result_paris/4_plus_wheelers_precision_vs_confidence.pdf}{Precision of 4+wheelers class = f(1-random forest confidence score). The horizontal line is the average confidence.}{fig:precision-vs-confidence} 
			\paragraph{Filtering: artificial increase of recall}
				The patch classifier is used on paris data set.
				The goal is to find all ground patches very fast.
				We focus on an area for illustration purpose. This area contains $3086$ patches, including $439$ ground patches.
				Patch classification finds $421$ ground patch, 
				with a recall of $92.7$\% .
				Using the found patch, all the surrounding patches (X,Y : $2$ \meter, Z : $0.5$ \meter ) are added to the result (few seconds).
				There are now $652$ patches in the result, and the recall is $100$\%.
				This means that from a filtering point of view, a complex classifier that would try to find ground points can be used on $652/3086=21\%$ of the data set, at the price of few seconds of computing, without any loss of information.
				
				\myimage{./illustrations/precision_vs_recall/ground_recall_increase.png}{Map of patch clustering result for ground. The classical result finds few extra patches that are not ground (blue), and misses some ground patches (red). Recall is increased by adding to the ground class all patches that are less than 2 meters in X,Y and 0.5 meter in Z around the found patches. Extra patches are much more numerous, but all the ground patches are found.}{fig:recall-increase}
					 	  